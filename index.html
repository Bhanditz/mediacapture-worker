<!DOCTYPE html>
<html>
  <head>
    <title>
      Media Capture Stream with Worker
    </title>
    <meta charset='utf-8'>
    <script src='https://www.w3.org/Tools/respec/respec-w3c-common' async
    class='remove'>
    </script>
    <script class='remove'>
      var respecConfig = {
          // specification status (e.g. WD, LCWD, WG-NOTE, etc.). If in doubt use ED.
          specStatus:           "ED",

          // the specification's short name, as in http://www.w3.org/TR/short-name/
          shortName:            "mediacapture-worker",

          // if your specification has a subtitle that goes below the main
          // formal title, define it here
          // subtitle   :  "an excellent document",

          // if you wish the publication date to be other than the last modification, set this
          // publishDate:  "2009-08-06",

          // if the specification's copyright date is a range of years, specify
          // the start date here:
          // copyrightStart: "2005"

          // if there is a previously published draft, uncomment this and set its YYYY-MM-DD date
          // and its maturity status
          // previousPublishDate:  "1977-03-15",
          // previousMaturity:  "WD",

          // if there a publicly available Editor's Draft, this is the link
           edDraftURI:           "https://w3c.github.io/mediacapture-worker/",

          // if this is a LCWD, uncomment and set the end of its review period
          // lcEnd: "2009-08-05",

          // editors, add as many as you like
          // only "name" is required
          editors:  [
              {
                  name:       "Chia-hung Tai"
    //              ,   url:        "http://example.org/"
              ,   mailto:     "ctai@mozilla.com"
              ,   company:    "Mozilla"
              ,   companyURL: "https://www.mozilla.org/en-US/foundation/moco/"
              },
              {
                  name:       "Robert O'Callahan"
    //              ,   url:        "http://example.org/"
              ,   company:    "Mozilla"
              ,   companyURL: "https://www.mozilla.org/en-US/foundation/moco/"
              },
              {
                  name:       "Tzuhao Kuo"
    //              ,   url:        "http://example.org/"
              ,   mailto:     "tkuo@mozilla.com"
              ,   company:    "Mozilla"
              ,   companyURL: "https://www.mozilla.org/en-US/foundation/moco/"
              },
              {
                  name:       "Anssi Kostiainen",
                  company:    "Intel",
                  companyURL: "http://www.intel.com/"
              },
          ],

          // name of the WG
          wg: [
                        "Web Real-Time Communication Working Group",
                        "Device APIs Working Group"
          ],
          // URI of the public WG page
          wgURI: [      "http://www.w3.org/2011/04/webrtc/",
                        "http://www.w3.org/2009/dap/"
          ],
          // name (without the @w3c.org) of the public mailing to which comments are due
          wgPublicList: "public-media-capture",

          // URI of the patent status for this WG, for Rec-track documents
          // !!!! IMPORTANT !!!!
          // This is important for Rec-track documents, do not copy a patent URI from a random
          // document unless you know what you're doing. If in doubt ask your friendly neighbourhood
          // Team Contact.
          wgPatentURI:  ["", ""],
          // !!!! IMPORTANT !!!! MAKE THE ABOVE BLINK IN YOUR HEAD
      };
    </script>
  </head>
  <body>
    <section id='abstract'>
      <p>
        This specification extends the <em>Media Capture and Streams</em>
        specification [[!GETUSERMEDIA]] to allow JavaScript developers to
        process video frame data in workers on the web applications.
      </p>
    </section>
    <section id='sotd'>
      <p>
        <strong>This document is not complete and is subject to change. Early
        experimentations are encouraged to allow the Media Capture Task Force
        to evolve the specification based on technical discussions within the
        Task Force, implementation experience gained from early
        implementations, and feedback from other groups and
        individuals.</strong>
      </p>
      <p>
        <strong>This specification is started in the Media Capture Task Force
        to get it under way in anticipation of the <a href=
        "http://www.w3.org/2015/07/timed-media-wg.html">Timed Media Working
        Group</a> creation. The intention is to transfer this specification to
        the Timed Media Working Group when the group is formed.</strong>
      </p>
      <p>
        <strong>This specification will use the <a href=
        "http://www.w3.org/Consortium/Legal/2015/copyright-software-and-document.html">
        W3C Software and Document license</a> to be consistent with the
        <a href="http://www.w3.org/2015/07/timed-media-wg.html#licensing">licensing
        terms</a> of the proposed Timed Media Working Group.</strong>
      </p>
    </section>
    <section>
      <h2>
        Introduction
      </h2>
      <p>
        The <em>Media Capture and Streams</em> specification provides a way to
        access to the video camera. But how to process the video frame data in
        JavaScript is not covered. By associating a worker with
        <a><code>MediaStreamTrack</code></a>s
        (<code>MediaStreamTrack.kind</code> must be "video"), the framework can
        dispatch a
        <a><code>VideoMonitorEvent</code></a>/<a><code>VideoProcessorEvent</code></a>
        to the worker frame by frame. Then JavaScript developer can write a
        video processing script which enables processing, analyzing of video
        data directly using JavaScript in a Worker thread.
      </p>
      <p>
        <img alt="The relationship between Worker and MediaStreamTrack" src=
        "images/Worker%20-%20FLOW.png" style="width:60%">
      </p>
      <p>
        The design principle of this specification is to provide a push-like
        mechanism for video processing. This design gives the Web developers no
        worry about any platform capability to full utilize the CPU usage. The
        Web developer no longer need to decide the FPS rate to grab the input
        video frame. The User Agent only dispatch the
        <a><code>VideoMonitorEvent</code></a>/<a><code>VideoProcessorEvent</code></a>
        when a new video frame arriving.
      </p>
    </section>
    <section>
      <h2>
        Use cases and requirements
      </h2>
      <p>
        This specification attempts to address the <a href=
        "https://wiki.mozilla.org/Project_FoxEye#Use_Cases">Use Cases and
        Requirements</a> for expanding the Web potentials to image processing
        and computer vision area.
      </p>
    </section>
    <section>
      <h2>
        Conformance
      </h2>
      <p>
        This specification defines conformance criteria that apply to a single
        product: the <dfn>user agent</dfn> that implements the interfaces that
        it contains.
      </p>
      <p>
        Implementations that use ECMAScript to implement the APIs defined in
        this specification must implement them in a manner consistent with the
        ECMAScript Bindings defined in the Web IDL specification [[!WEBIDL]],
        as this specification uses that specification and terminology.
      </p>
      <p>
        The key words "MUST", "MUST NOT", "REQUIRED", "SHOULD", "SHOULD NOT",
        "RECOMMENDED", "MAY", and "OPTIONAL" in the normative parts of this
        document are to be interpreted as described in RFC2119. For
        readability, these words do not appear in all uppercase letters in this
        specification. [[!RFC2119]]
      </p>
    </section>
    <section>
      <h2>
        Dependencies
      </h2>
      <p>
        The <code><a href=
        "http://w3c.github.io/mediacapture-main/getusermedia.html#idl-def-MediaStreamTrack">
        <dfn>MediaStreamTrack</dfn></a></code> and <code><a href=
        "http://w3c.github.io/mediacapture-main/getusermedia.html#idl-def-MediaStream">
        <dfn>MediaStream</dfn></a></code> interfaces this specification extends
        and the <a href=
        "http://w3c.github.io/mediacapture-main/#dfn-source"><dfn>source</dfn></a>
        concept are defined in [[!GETUSERMEDIA]].
      </p>
      <p>
        The <code><a href=
        "http://www.w3.org/TR/html51/webappapis.html#imagebitmap"><dfn>ImageBitmap</dfn></a></code>
        interface and <code><a href=
        "http://www.w3.org/TR/html51/webappapis.html#imagebitmapfactories"><dfn>
        ImageBitmapFactories</dfn></a></code> interface this specification
        extends are defined in [[!HTML51]].
      </p>
      <p>
        The <code><a href=
        "http://www.w3.org/TR/WebIDL-1/#common-BufferSource"><dfn>BufferSource</dfn></a></code>
        is defined in [[!WEBIDL]]
      </p>
      <p>
        The <code><a href=
        "http://www.ecma-international.org/ecma-262/6.0/#sec-promise-objects">Promise</a></code>
        object is defined in [[!ECMASCRIPT]].
      </p>
      <p>
        The following concepts and interfaces are defined in [[!HTML]]:
      </p>
      <ul>
        <li>
          <dfn><a href=
          "https://html.spec.whatwg.org/multipage/webappapis.html#event-handlers">
          Event handler</a></dfn>
        </li>
        <li>
          <dfn><a href=
          "https://html.spec.whatwg.org/multipage/webappapis.html#event-handler-event-type">
          event handler event type</a></dfn>
        </li>
        <li>
          <dfn><a href=
          "https://html.spec.whatwg.org/multipage/webappapis.html#event-handler-idl-attributes">
          event handler IDL attribute</a></dfn>
        </li>
        <li>
          <dfn><a href=
          "https://html.spec.whatwg.org/multipage/infrastructure.html#in-parallel">
          in parallel</a></dfn>
        </li>
      </ul>
    </section>
    <section>
      <h2>
        Terminology
      </h2>
      <p>
        There are two kinds of <dfn data-lt="video worker">video workers</dfn>;
        <a data-lt="monitoring video worker">monitoring video workers</a>, and
        <a data-lt="processing video worker">processing video workers</a>:
      </p>
      <dl>
        <dt>
          <a>Monitoring video worker</a>
        </dt>
        <dd>
          Captures an <a>input frame</a> from an <a>input media stream
          track</a>. A <a>video worker</a> is said to be a <dfn>monitoring
          video worker</dfn>, if the <a>video worker</a> is associated with an
          <a>input media stream track</a>. <a data-lt=
          "video monitor event">Video monitor events</a> are dispatched at a
          <a>monitoring video worker</a>.
        </dd>
        <dt>
          <a>Processing video worker</a>
        </dt>
        <dd>
          Similarly captures an <a>input frame</a> from an <a>input media
          stream track</a>, and in addition, processes the provided <a>output
          frame</a> for use as the <a>source</a> for the <a>output media stream
          track</a>. A <a>video worker</a> is said to be a <dfn>processing
          video worker</dfn>, if the <a>video worker</a> is associated with
          both an <a>input media stream track</a> and an <a>output media stream
          track</a>. <a data-lt="video processor event">Video processor
          events</a> are dispatched at a <a>processing video worker</a>.
        </dd>
      </dl>
      <p link-for="MediaStreamTrack">
        An <dfn>input frame</dfn> for a <a>video worker</a> <var>w</var> is an
        <a>ImageBitmap</a> object representing a frame sourced from the
        <a>input media stream track</a> associated with <var>w</var>.
      </p>
      <p link-for="VideoProcessorEvent">
        An <dfn>output frame</dfn> for a <a>video worker</a> <var>w</var> is an
        <a>ImageBitmap</a> object assigned to the <a>outputImageBitmap</a>
        attribute of the <a>VideoProcessorEvent</a> dispatched at <var>w</var>.
      </p>
      <p link-for="MediaStreamTrack">
        The <dfn>input media stream track</dfn> for a <a>video worker</a>
        <var>w</var> is a <a>MediaStreamTrack</a> object passed as the first
        argument to the <code><a>addVideoMonitor</a>()</code> or
        <code><a>addVideoProcessor</a>()</code> method on <var>w</var>.
      </p>
      <p link-for="MediaStreamTrack">
        The <dfn>output media stream track</dfn> for a <a>video worker</a>
        <var>w</var> is a <a>MediaStreamTrack</a> object returned by the
        invocation of the <code><a>addVideoProcessor</a>()</code> method on
        <var>w</var>.
      </p>
      <p>
        There are two kinds of <dfn data-lt="video worker event">video worker
        events</dfn>:
      </p>
      <dl>
        <dt>
          <dfn data-lt="video monitor event">Video monitor events</dfn>
        </dt>
        <dd>
          These events are represented by <a>VideoMonitorEvent</a> objects that
          are dispatched at a <a>monitoring video worker</a> and provide read
          access to <a>input frame</a> data.
        </dd>
      </dl>
      <dl>
        <dt>
          <dfn data-lt="video processor event">Video processor events</dfn>
        </dt>
        <dd>
          These events are represented by <a>VideoProcessorEvent</a> objects
          that are dispatched at a <a>processing video worker</a> and provide
          read access to <a>input frame</a> data and write access to <a>output
          frame</a> data.
        </dd>
      </dl>
    </section>
    <section>
      <h2>
        Extensions
      </h2>
      <section>
        <h2>
          <code>DedicatedWorkerGlobalScope</code> interface
        </h2>
        <p link-for="DedicatedWorkerGlobalScope">
          To enable generating, processing, and analyzing of video data
          directly using JavaScript in a worker, the
          <a>DedicatedWorkerGlobalScope</a> interface is extended with an
          <a>onvideomonitor</a> and <a>onvideoprocess</a> <a>event handler IDL
          attribute</a>. <a data-lt="video monitor event">Video monitor
          events</a> and <a data-lt="video processor event">video processor
          events</a> are dispatched at the <a>DedicatedWorkerGlobalScope</a>
          object.
        </p>
        <pre class="idl">
          partial interface DedicatedWorkerGlobalScope {
                          attribute EventHandler onvideomonitor;
                          attribute EventHandler onvideoprocess;
          };
        </pre>
        <div dfn-for="DedicatedWorkerGlobalScope">
          <p>
            The following are the <a data-lt="event handler">event handlers</a>
            (and their corresponding <a data-lt=
            "event handler event type">event handler event types</a>) that must
            be supported, as event handler IDL attributes, by objects
            implementing the <a>DedicatedWorkerGlobalScope</a> interface:
          </p>
          <table class="simple">
            <thead>
              <tr>
                <th>
                  Event handlers
                </th>
                <th>
                  <a>Event handler event type</a>
                </th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <dfn><code>onvideomonitor</code></dfn>
                </td>
                <td>
                  <dfn><code>videomonitor</code></dfn>
                </td>
              </tr>
              <tr>
                <td>
                  <dfn><code>onvideoprocess</code></dfn>
                </td>
                <td>
                  <dfn><code>videoprocess</code></dfn>
                </td>
              </tr>
            </tbody>
          </table>
        </div>
        <section>
          <h2>
            Video worker event firing
          </h2>
          <p>
            To <dfn>fire a video worker event</dfn> named <var>e</var>, the
            user agent must run the following steps:
          </p>
          <ol>
            <li link-for="VideoMonitorEvent">If <var>e</var> is
            <a>videomonitor</a>, create a <a>VideoMonitorEvent</a>, initialize
            it to have a given name <var>e</var>, to not bubble, to not be
            cancelable.
            </li>
            <li link-for="VideoProcessorEvent">If <var>e</var> is
            <a>videoprocess</a>, create a <a>VideoProcessorEvent</a>,
            initialize it to have a given name <var>e</var>, to not bubble, to
            not be cancelable.
            </li>
            <li link-for="VideoMonitorEvent">Initialize the <a>trackId</a>
            attribute to the value of the <code>id</code> attribute of the <a>
              input media stream track</a>.
            </li><!-- TODO -->
            <li link-for="VideoMonitorEvent">Initialize the <a>playbackTime</a>
            attribute to the value of the <code>currentTime</code> attribute of
            the <a>MediaStream</a> that contains the <a>input media stream
            track</a>.
            </li>
            <li link-for="VideoMonitorEvent">Initialize the
            <a>inputImageBitmap</a> attribute to the bitmap data of the
            <a>input media stream track</a>'s video frame at the current stream
            position at <a>playbackTime</a>.
            </li>
            <li>
              <!-- TODO: clarify the definition of targets -->
              Let <var>associated video workers</var> be the list of
              <a data-lt="video worker">video workers</a> that share the same
              <a>input media stream track</a>.
            </li>
            <li>If <var>e</var> is <a>videomonitor</a>, dispatch the newly
            created <a>VideoMonitorEvent</a> object at each <var>target</var>.
            </li>
            <li link-for="VideoProcessorEvent">If <var>e</var> is
            <a>videoprocess</a>, initialize the <a>outputImageBitmap</a>
            attribute to null, and dispatch the newly created
            <a>VideoProcessorEvent</a> object at each <var>target</var>.
            </li>
          </ol>
        </section>
      </section>
      <section>
        <h2>
          <code id="event-videomonitorevent">VideoMonitorEvent</code> interface
        </h2>
        <p link-for="VideoMonitorEvent">
          The <a>video monitor event</a> contains an <a>input frame</a> and its
          metadata originating from the <a>input media stream track</a>. It
          uses the <a>VideoMonitorEvent</a> interface for its
          <a>videomonitor</a> events:
        </p>
        <pre class="idl">
          [Constructor(DOMString type, optional VideoMonitorEventInit videoMonitorEventInitDict), Exposed=Worker]
          interface VideoMonitorEvent : Event {
              readonly    attribute DOMString   trackId;
              readonly    attribute double      playbackTime;
              readonly    attribute ImageBitmap? inputImageBitmap;
          };
          dictionary VideoMonitorEventInit : EventInit {
              required DOMString    trackId;
              required double       playbackTime;
              required ImageBitmap? inputImageBitmap;
          };
        </pre>
        <div dfn-for="VideoMonitorEvent">
          <p>
            The <dfn>trackId</dfn> attribute must return the value it was
            initialized to. When the object is created, this attribute must be
            initialized to the empty string. It represents the identifier that
            is shared with the <a>video worker</a> and its <a>input media
            stream track</a>.
          </p>
          <p>
            <!-- TODO: is this current stream position? -->
             The <dfn>playbackTime</dfn> attribute must return the value it was
            initialized to. When the object is created, this attribute must be
            initialized to zero. It represents the current stream position, in
            seconds.
          </p>
          <p>
            The <dfn>inputImageBitmap</dfn> attribute must return the value it
            was initialized to. When the object is created, this attribute must
            be initialized to null. It represents the <a>ImageBitmap</a> object
            whose bitmap data is provided by the <a>input media stream
            track</a>.
          </p>
        </div>
        <p>
          When a user agent is required to <dfn>fire a video monitor
          event</dfn>, it must <a>fire a video worker event</a> named
          <a>videomonitor</a>.
        </p>
      </section>
      <section>
        <h2>
          <code id="event-videoprocessorevent">VideoProcessorEvent</code>
          interface
        </h2>
        <p link-for="VideoProcessorEvent">
          The <a>video processor event</a> inherits from the <a>video monitor
          event</a>, and in addition provides means to programmatically
          construct an <a>output frame</a> for the <a>output media stream
          track</a>. It uses the <a>VideoProcessorEvent</a> interface for its
          <a>videoprocess</a> events:
        </p>
        <pre class="idl">
          [Constructor(DOMString type, optional VideoProcessorEventInit videoProcessorEventInitDict),
           Exposed=Worker]
          interface VideoProcessorEvent : VideoMonitorEvent {
                          attribute ImageBitmap? outputImageBitmap;
          };

          dictionary VideoProcessorEventInit : VideoMonitorEventInit {
              required ImageBitmap? outputImageBitmap;
          };
        </pre>
        <div dfn-for="VideoProcessorEvent">
          <p>
            The <dfn>outputImageBitmap</dfn> attribute must return the value it
            was initialized to. When the object is created, this attribute must
            be initialized to null. It represents the <a>ImageBitmap</a> object
            which on setting, must cause the user agent to run the following
            steps:
          </p>
        </div>
        <ol link-for="VideoProcessorEvent">
          <li>Let <var>output bitmap</var> be the <a>ImageBitmap</a> object
          assigned to the <a>outputImageBitmap</a> attribute.
          </li>
          <li link-for="MediaStreamTrack">Let <var>output media stream
          track</var> be the <a>MediaStreamTrack</a> returned by the <code><a>
            addVideoProcessor</a>()</code> method.
          </li>
          <li>
            <!-- TODO: this should be defined better. -->
            Set <var>output bitmap</var> as the <a>source</a> of the
            <var>output media stream track</var>.
          </li>
        </ol>
        <p>
          When a user agent is required to <dfn>fire a video processor
          event</dfn>, it must <a>fire a video worker event</a> named
          <a>videoprocess</a>.
        </p>
        <div class="note">
          <p>
            Ideally the <code>MediaStreamTrack</code> should dispatch each
            video frame through <a>VideoProcessorEvent</a>. But sometimes the
            worker thread could not process the frame in time. So the
            implementation could skip the frame to avoid high memory footprint.
            In such case, we might not be able to process every frame in a real
            time <code>MediaStream</code>.
          </p>
        </div>
      </section>
      <section>
        <h2>
          <code>MediaStreamTrack</code> interface
        </h2>
        <pre class="idl">
          partial interface MediaStreamTrack {
              Promise&lt;void&gt;             addVideoMonitor(Worker worker);
              Promise&lt;void&gt;             removeVideoMonitor(Worker worker);
              Promise&lt;MediaStreamTrack&gt; addVideoProcessor(Worker worker);
              Promise&lt;void&gt;             removeVideoProcessor();
          };
        </pre>
        <div dfn-for="MediaStreamTrack">
          <p>
            The <code><dfn>addVideoMonitor</dfn>()</code> method, when invoked,
            must run these steps:
          </p>
          <ol>
            <li>Let <var>promise</var> be a new promise.
            </li>
            <li>Run these substeps <a>in parallel</a>:
              <ol>
                <li>Let <var>worker</var> be the first method argument.
                </li>
                <li>Let <var>track</var> be the <a>MediaStreamTrack</a> object
                on which the method was invoked. (This is the <a>input media
                stream track</a>.)
                </li>
                <li>Associate <var>worker</var> with <a>input media stream
                track</a> <var>track</var>.
                </li>
                <li>Resolve <var>promise</var> with undefined.
                </li>
              </ol>
            </li>
            <li>Return <var>promise</var>.
            </li><!-- TODO: when do we want to reject the promise? -->
          </ol>
          <p>
            The <code><dfn>removeVideoMonitor</dfn>()</code> method, when
            invoked, must run these steps:
          </p>
          <ol>
            <li>Let <var>promise</var> be a new promise.
            </li>
            <li>Run these substeps <a>in parallel</a>:
              <ol>
                <li>Let <var>worker</var> be the first method argument.
                </li>
                <li>Let <var>track</var> be the <a>MediaStreamTrack</a> object
                on which the method was invoked.
                </li>
                <li>If there exists an association between <var>worker</var>
                and <var>track</var>, break that association and resolve <var>
                  promise</var> with undefined.
                </li>
                <li>Otherwise, reject <var>promise</var> with
                <code>NotFoundError</code>.
                </li>
              </ol>
            </li>
            <li>Return <var>promise</var>.
            </li>
          </ol>
          <p>
            The <code><dfn>addVideoProcessor</dfn>()</code> method, when
            invoked, must run these steps:
          </p>
          <ol>
            <li>Let <var>promise</var> be a new promise.
            </li>
            <li>Run these substeps <a>in parallel</a>:
              <ol>
                <li>Let <var>worker</var> be the first method argument.
                </li>
                <li>Let <var>track</var> be the <a>MediaStreamTrack</a> object
                on which the method was invoked. (This is the <a>input media
                stream track</a>.)
                </li>
                <li>If there exists an association between <var>worker</var>
                and <var>track</var>, reject promise with
                <code>QuotaExceededError</code>.
                  <div class="note">
                    A single <a>input media stream track</a> is associated with
                    at most one <a>processing video worker</a> at a time.
                  </div>
                </li>
                <li>Associate <var>worker</var> with <a>input media stream
                track</a> <var>track</var>.
                </li>
                <li>Let <var>new track</var> be a newly created
                <a>MediaStreamTrack</a> object. (This is the <a>output media
                stream track</a>.)
                </li>
                <li>Associate <var>new track</var> as the <a>output media
                stream track</a> for <var>worker</var>.
                </li>
                <li>Resolve <var>promise</var> with <var>new track</var>.
                </li>
              </ol>
            </li>
            <li>Return <var>promise</var>.
            </li>
          </ol>
          <p>
            The <code><dfn>removeVideoProcessor</dfn>()</code> method, when
            invoked, must run these steps:
          </p>
          <ol>
            <li>Let <var>promise</var> be a new promise.
            </li>
            <li>Run these substeps <a>in parallel</a>:
              <ol>
                <li>Let <var>worker</var> be the first method argument.
                </li>
                <li>Let <var>track</var> be the <a>MediaStreamTrack</a> object
                on which the method was invoked.
                </li>
                <li>If there exists an association between <var>worker</var>
                and <var>track</var>, break that association and resolve <var>
                  promise</var> with undefined.
                </li>
                <li>Otherwise, reject <var>promise</var> with
                <code>NotFoundError</code>.
                </li>
              </ol>
            </li>
            <li>Return <var>promise</var>.
            </li>
          </ol>
        </div>
      </section>
    </section>
    <section id='imagebitmap-extensions'>
      <h2>
        ImageBitmap extensions
      </h2>
      <div class="note">
        <p>
          The <a>ImageBitmap</a> interface is originally designed as a pure
          opaque handler to an image data buffer inside a browser so that how
          the browser stores the buffer is uknown to users and optimized to
          platforms. In this specification, we chooses <a>ImageBitmap</a>
          (instead of <code>ImageData</code>) as the container of video frames
          because the decoded video frame data might exist in either CPU or GPU
          memory which perfectly matches the nature of
          <a><code>ImageBitmap</code></a> as an opaque handler.
        </p>
      </div>
      <p>
        Considering how would developers process video frames, there are two
        possible approaches, via pure JavaScript(/asm.js) code or via WebGL.
      </p>
      <ol>
        <li>If developers use JavaScript(/asm.js) to process the frames, then
        the <a>ImageBitmap</a> interface needs to be extended with APIs for
        developers to access its underlying data and there should also be a way
        for developers to create an <a>ImageBitmap</a> from the processed data.
        </li>
        <li>If developers use WebGL, then WebGL needs to be extended so that
        developers can pass an <a>ImageBitmap</a> into the WebGL context and
        the browser will handle how to upload the raw image data into the GPU
        memory. Possibly, the data is already in the GPU memory so that the
        operation could be very efficient.
        </li>
      </ol>
      <p>
        In this specification, the original <a>ImageBitmap</a> interface is
        extended with three methods to let developers read data from an
        <a>ImageBitmap</a> object into a given <a>BufferSource</a> in a set of
        supported <a>ImageFormat</a>s and two interfaces,
        <a>ImageFormatPixelLayout</a> and <a>ChannelPixelLayout</a>, are
        proposed to work with the extend <a>ImageBitmap</a> methods to describe
        how the accessed image data is arranged in memory. Also, the
        <a>ImageBitmapFactories</a> interface is extended to let developers
        create an <a>ImageBitmap</a> object from a given <a>BufferSource</a>.
      </p>
      <section id='imageformat'>
        <h2>
          Image format
        </h2>
        <p>
          An image or a video frame is conceptually a two-dimentional array of
          data and each element in the array is called a <dfn>pixel</dfn>. The
          <a data-lt="pixel">pixels</a> are usually stored in a one-dimensional
          array and could be arranged in a variety of <a data-lt=
          "image format">image formats</a>. Developers need to know how the
          <a data-lt="pixel">pixels</a> are formatted so that they are able to
          process them.
        </p>
        <p>
          The <a>image format</a> describes how pixels in an image are
          arranged. A single pixel has at least one, but usually multiple
          <dfn data-lt="pixel value">pixel values</dfn>. The range of a
          <a>pixel value</a> varies, which means different <a data-lt=
          "image format">image formats</a> use different <a data-lt=
          "data type">data types</a> to store a single <a>pixel value</a>.
        </p>
        <p>
          The most frequently used <a>data type</a> is 8-bit unsigned integer
          whose range is from 0 to 255, others could be 16-bit integer or
          32-bit floating points and so forth. The number of <a data-lt=
          "pixel value">pixel values</a> of a single <a>pixel</a> is called the
          <dfn data-lt="number of channel">number of channels</dfn> of the
          <a>image format</a>. Multiple <a data-lt="pixel value">pixel
          values</a> of a <a>pixel</a> are used together to describe the
          captured property which could be color or depth information. For
          example, if the data is a color image in RGB color space, then it is
          a three-channel <a>image format</a> and a <a>pixel</a> is described
          by R, G and B three pixel values with range from 0 to 255. As another
          example, if the data is a gray image, then it is a single-channel
          <a>image format</a> with 8-bit unsigned integer <a>data type</a> and
          the <a>pixel value</a> describes the gray scale. For depth data, it
          is a single channel <a>image format</a> too, but the <a>data type</a>
          is 16-bit unsigned integer and the <a>pixel value</a> is the depth
          level.
        </p>
        <p>
          For those <a data-lt="image format">image formats</a> whose
          <a data-lt="pixel">pixels</a> contain multiple <a data-lt=
          "pixel value">pixel values</a>, the <a data-lt="pixel value">pixel
          values</a> might be arranged in one of the following ways:
        </p>
        <ol>
          <li>
            <dfn>Planar pixel layout</dfn>: each <a>channel</a> has its
            <a data-lt="pixel value">pixel values</a> stored consecutively in
            separated buffers (a.k.a. planes) and then all channel buffers are
            stored consecutively in memory. (Ex:
            RRRRRR......GGGGGG......BBBBBB......)
          </li>
          <li>
            <dfn>Interleaving pixel layout</dfn>: each <a>pixel</a> has its
            <a data-lt="pixel value">pixel values</a> from all <a data-lt=
            "channel">channels</a> stored together and interleaves all
            <a data-lt="channel">channels</a>. (Ex: RGBRGBRGBRGBRGB......)
          </li>
        </ol>
        <div class="note">
          <p>
            <a data-lt="image format">Image formats</a> that belong to the same
            color space might have different <a data-lt="pixel layout">pixel
            layouts</a>.
          </p>
        </div>
        <section id='imageformat-enumaration'>
          <h2>
            <code>ImageFormat</code> enumeration
          </h2>
          <p>
            The <a>ImageFormat</a> enumeration is used to select the <dfn>image
            format</dfn> for the <a>ImageFormatPixelLayout</a>. The
            <a>ImageBitmap</a> extensions defined in this specification use
            this enumeration to negotiate the <a>image format</a> while
            accessing the underlying data of <a>ImageBitmap</a> and creating a
            new <a>ImageBitmap</a>.
          </p>
          <div class="note">
            <p>
              We need to elaborate this list for standardization.
            </p>
          </div>
          <pre class="idl">
            enum ImageFormat {
                "RGBA32",
                "BGRA32",
                "RGB24",
                "BGR24",
                "GRAY8",
                "YUV444P",
                "YUV422P",
                "YUV420P",
                "YUV420SP_NV12",
                "YUV420SP_NV21",
                "HSV",
                "Lab",
                "DEPTH"
            };
          </pre>
          <table class="simple" dfn-for="ImageFormat">
            <tr>
              <th>
                ImageFormat
              </th>
              <th>
                Channel order
              </th>
              <th>
                Channel size
              </th>
              <th>
                Pixel layout
              </th>
              <th>
                Data type
              </th>
            </tr>
            <tr>
              <td>
                <code><dfn>RGBA32</dfn></code>
              </td>
              <td>
                R, G, B, A
              </td>
              <td>
                full rgba-channels
              </td>
              <td>
                interleaving rgba-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>BGRA32</dfn></code>
              </td>
              <td>
                B, G, R, A
              </td>
              <td>
                full bgra-channels
              </td>
              <td>
                interleaving bgra-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>RGB24</dfn></code>
              </td>
              <td>
                R, G, B
              </td>
              <td>
                full rgb-channels
              </td>
              <td>
                interleaving rgb-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>BGR24</dfn></code>
              </td>
              <td>
                B, G, R
              </td>
              <td>
                full bgr-channels
              </td>
              <td>
                interleaving bgr-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>GRAY8</dfn></code>
              </td>
              <td>
                GRAY
              </td>
              <td>
                full gray-channel
              </td>
              <td>
                planar gray-channel
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>YUV444P</dfn></code>
              </td>
              <td>
                Y, U, V
              </td>
              <td>
                full yuv-channels
              </td>
              <td>
                planar yuv-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>YUV422P</dfn></code>
              </td>
              <td>
                Y, U, V
              </td>
              <td>
                full y-channel, half uv-channels
              </td>
              <td>
                planar yuv-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>YUV420P</dfn></code>
              </td>
              <td>
                Y, U, V
              </td>
              <td>
                full y-channel, quarter uv-channels
              </td>
              <td>
                planar yuv-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>YUV420SP_NV12</dfn></code>
              </td>
              <td>
                Y, U, V
              </td>
              <td>
                full y-channel, quarter uv-channels
              </td>
              <td>
                planar y-channel, interleaving uv-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>YUV420SP_NV21</dfn></code>
              </td>
              <td>
                Y, V, U
              </td>
              <td>
                full y-channel, quarter uv-channels
              </td>
              <td>
                planar y-channel, interleaving vu-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>HSV</dfn></code>
              </td>
              <td>
                H, S, V
              </td>
              <td>
                full hsv-channels
              </td>
              <td>
                interleaving hsv-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>Lab</dfn></code>
              </td>
              <td>
                l, a, b
              </td>
              <td>
                full lab-channels
              </td>
              <td>
                interleaving lab-channels
              </td>
              <td>
                8-bit unsigned integer
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>DEPTH</dfn></code>
              </td>
              <td>
                DEPTH
              </td>
              <td>
                full depth-channel
              </td>
              <td>
                planar depth-channel
              </td>
              <td>
                16-bit unsigned integer
              </td>
            </tr>
          </table>
        </section>
        <section id='datatype-enumeration'>
          <h2>
            <code>DataType</code> enumeration
          </h2>
          <p>
            The <a>DataType</a> enumeration is used to select the <dfn>channel
            data type</dfn> that is used to store a single <a>pixel value</a>.
          </p>
          <pre class="idl">
            enum DataType {
                "uint8",
                "int8",
                "uint16",
                "int16",
                "uint32",
                "int32",
                "float32",
                "float64"
            };
          </pre>
          <table class="simple" dfn-for="DataType">
            <tr>
              <th>
                DataType
              </th>
              <th>
                description
              </th>
            </tr>
            <tr>
              <td>
                <code><dfn>uint8</dfn></code>
              </td>
              <td>
                8-bit unsigned integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>int8</dfn></code>
              </td>
              <td>
                8-bit integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>uint16</dfn></code>
              </td>
              <td>
                16-bit unsigned integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>int16</dfn></code>
              </td>
              <td>
                16-bit integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>uint32</dfn></code>
              </td>
              <td>
                32-bit unsigned integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>int32</dfn></code>
              </td>
              <td>
                32-bit integer.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>float32</dfn></code>
              </td>
              <td>
                32-bit IEEE floating point number.
              </td>
            </tr>
            <tr>
              <td>
                <code><dfn>float64</dfn></code>
              </td>
              <td>
                64-bit IEEE floating point number.
              </td>
            </tr>
          </table>
        </section>
      </section>
      <section id='pixellayout'>
        <h2>
          Pixel layout
        </h2>
        <p>
          Two interfaces, <a>ImageFormatPixelLayout</a> and
          <a>ChannelPixelLayout</a>, together generalize the variety of
          <a data-lt="pixel layout">pixel layouts</a> among <a data-lt=
          "image format">image formats</a>.
        </p>
        <p>
          The <a>ImageFormatPixelLayout</a> represents the <dfn>pixel
          layout</dfn> of a certain <a data-lt="image format">image format</a>.
          Since an <a>image format</a> is composed of at least one
          <dfn>channel</dfn>, an <a>ImageFormatPixelLayout</a> object contains
          at least one <a>ChannelPixelLayout</a> object. 
          <!-- TODO: more elaborate description of a channel needed -->
        </p>
        <p>
          Although an image or a video frame is a two-dimensional structure,
          its data is usually stored in a one-dimensional array in the
          row-major way and each <a>channel</a> describes how <a data-lt=
          "pixel value">pixel values</a> are arranged in the one dimensional
          array buffer.
        </p>
        <p>
          A <a>channel</a> has an associated <dfn>offset</dfn> that denotes the
          beginning position of the <a>channel</a>'s data relative to the given
          <a>BufferSource</a> parameter of the
          <code><a>mapDataInto</a>()</code> method.
        </p>
        <p>
          A <a>channel</a> has an associated <dfn>width</dfn> and
          <dfn>height</dfn> that denote the width and height of the
          <a>channel</a> respectively. Each <a>channel</a> in an <a>image
          format</a> may have different height and width.
        </p>
        <p>
          A <a>channel</a> has an associated <dfn>data type</dfn> used to store
          one single <a>pixel value</a>.
        </p>
        <p>
          A <a>channel</a> has an associated <dfn>stride</dfn> that is the
          number of bytes between the beginning two consecutive rows in memory.
          (The total bytes of each row plus the padding bytes of each raw.)
        </p>
        <p>
          A <a>channel</a> has an associated <dfn>skip</dfn> value. The value
          is zero for the <a>planar pixel layout</a>, and a positive integer
          for the <a>interleaving pixel layout</a>. (Describes how many bytes
          there are between two adjacent pixel values in this channel.)
        </p>
        <pre class='example highlight'>
          Example1: RGBA image, width = 620, height = 480, stride = 2560

          chanel_r: offset = 0, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3
          chanel_g: offset = 1, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3
          chanel_b: offset = 2, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3
          chanel_a: offset = 3, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3

                  &lt;---------------------------- stride ----------------------------&gt;
                  &lt;---------------------- width x 4 ----------------------&gt;
          [index] 01234   8   12  16  20  24  28                           2479    2559
                  |||||---|---|---|---|---|---|----------------------------|-------|
          [data]  RGBARGBARGBARGBARGBAR___R___R...                         A%%%%%%%%
          [data]  RGBARGBARGBARGBARGBAR___R___R...                         A%%%%%%%%
          [data]  RGBARGBARGBARGBARGBAR___R___R...                         A%%%%%%%%
                       ^^^
                       r-skip
        </pre>
        <pre class='example highlight'>
          Example2: YUV420P image, width = 620, height = 480, stride = 640

          chanel_y: offset = 0, width = 620, height = 480, stride = 640, skip = 0
          chanel_u: offset = 307200, width = 310, height = 240, data type = uint8, stride = 320, skip = 0
          chanel_v: offset = 384000, width = 310, height = 240, data type = uint8, stride = 320, skip = 0

                  &lt;--------------------------- y-stride ---------------------------&gt;
                  &lt;----------------------- y-width -----------------------&gt;
          [index] 012345                                                  619      639
                  ||||||--------------------------------------------------|--------|
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                        Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                        Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                        Y%%%%%%%%%
          [data]  ......
                  &lt;-------- u-stride ----------&gt;
                  &lt;----- u-width -----&gt;
          [index] 307200              307509   307519
                  |-------------------|--------|
          [data]  UUUUUUUUUU...       U%%%%%%%%%
          [data]  UUUUUUUUUU...       U%%%%%%%%%
          [data]  UUUUUUUUUU...       U%%%%%%%%%
          [data]  ......
                  &lt;-------- v-stride ----------&gt;
                  &lt;- --- v-width -----&gt;
          [index] 384000              384309   384319
                  |-------------------|--------|
          [data]  VVVVVVVVVV...       V%%%%%%%%%
          [data]  VVVVVVVVVV...       V%%%%%%%%%
          [data]  VVVVVVVVVV...       V%%%%%%%%%
          [data]  ......
        </pre>
        <pre class='example highlight'>
          Example3: YUV420SP_NV12 image, width = 620, height = 480, stride = 640

          chanel_y: offset = 0, width = 620, height = 480, stride = 640, skip = 0
          chanel_u: offset = 307200, width = 310, height = 240, data type = uint8, stride = 640, skip = 1
          chanel_v: offset = 307201, width = 310, height = 240, data type = uint8, stride = 640, skip = 1

                  &lt;--------------------------- y-stride --------------------------&gt;
                  &lt;----------------------- y-width ----------------------&gt;
          [index] 012345                                                 619      639
                  ||||||-------------------------------------------------|--------|
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                       Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                       Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                       Y%%%%%%%%%
          [data]  ......
                  &lt;--------------------- u-stride / v-stride --------------------&gt;
                  &lt;------------------ u-width + v-width -----------------&gt;
          [index] 307200(u-offset)                                       307819  307839
                  |------------------------------------------------------|-------|
          [index] |307201(v-offset)                                      |307820 |
                  ||-----------------------------------------------------||------|
          [data]  UVUVUVUVUVUVUVUVUVUVUVUVUVUVUV...                      UV%%%%%%%
          [data]  UVUVUVUVUVUVUVUVUVUVUVUVUVUVUV...                      UV%%%%%%%
          [data]  UVUVUVUVUVUVUVUVUVUVUVUVUVUVUV...                      UV%%%%%%%
                   ^            ^
                  u-skip        v-skip
        </pre>
        <pre class='example highlight'>
          Example4: DEPTH image, width = 640, height = 480, stride = 1280

          chanel_d: offset = 0, width = 640, height = 480, data type = uint16, stride = 1280, skip = 0

                  &lt;----------------------- d-stride ----------------------&gt;
                  &lt;----------------------- d-width -----------------------&gt;
          [index] 012345                                                  1280
                  ||||||--------------------------------------------------|
          [data]  DDDDDDDDDDDDDDDDDDDDDDDDDDDDD...                        D
          [data]  DDDDDDDDDDDDDDDDDDDDDDDDDDDDD...                        D
          [data]  DDDDDDDDDDDDDDDDDDDDDDDDDDDDD...                        D
          [data]  ......
        </pre>
        <section id='imageformatpixellayout-interface'>
          <h2>
            <code>ImageFormatPixelLayout</code> interface
          </h2>
          <p>
            Each <a>channel</a> is represented by an <a>ChannelPixelLayout</a>
            object.
          </p>
          <pre class="idl">
            [Exposed=(Window,Worker)]
            interface ChannelPixelLayout {
                readonly    attribute unsigned long offset;
                readonly    attribute unsigned long width;
                readonly    attribute unsigned long height;
                readonly    attribute DataType      dataType;
                readonly    attribute unsigned long stride;
                readonly    attribute unsigned long skip;
            };
          </pre>
          <div dfn-for="ChannelPixelLayout">
            <p>
              The <dfn>offset</dfn> attribute represents the <a>channel</a>'s
              <a>offset</a>.
            </p>
            <p>
              The <dfn>width</dfn> attribute represents the <a>width</a> of the
              <a>channel</a>. (Channels in an image format may have different
              width.)
            </p>
            <p>
              The <dfn>height</dfn> attribute represents the <a>height</a> of
              the <a>channel</a>. (Channels in an image format may have
              different height.)
            </p>
            <p>
              The <dfn>dataType</dfn> attribute must return the <a>data
              type</a> of the <a>channel</a>, one of the <a data-lt=
              "channel data type">channel data types</a>.
            </p>
            <p>
              The <dfn>stride</dfn> attribute represents the <a>stride</a> of
              the <a>channel</a>.
            </p>
            <p>
              The <dfn>skip</dfn> attribute represents the <a>skip</a> value
              for the <a>channel</a>.
            </p>
          </div>
        </section>
        <section id='channelpixellayout-interface'>
          <h2>
            <code>ChannelPixelLayout</code> interface
          </h2>
          <pre class="idl">
            [Exposed=(Window,Worker)]
            interface ImageFormatPixelLayout {
                readonly    attribute ChannelPixelLayout[] channels;
            };
          </pre>
          <p dfn-for="ImageFormatPixelLayout">
            The <dfn>channels</dfn> attribute must return a read only array of
            <a>ChannelPixelLayout</a> objects. The returned array represents
            <a data-lt="channel">channels</a> of the <a>image format</a>. Each
            <a>image format</a> has at least one <a>channel</a>.
          </p>
        </section>
      </section>
      <section id='imagebitmap-interface-extensions'>
        <h2>
          <code>ImageBitmap</code> interface
        </h2>
        <dl class="idl" title=
        "[Exposed=(Window,Worker)] partial interface ImageBitmap">
          <dt>
            ImageFormat findOptimalFormat()
          </dt>
          <dd>
            <p>
              Find the best image format for receiving data.
            </p>
            <p>
              Return one of the <code>possibleFormats</code> or the empty
              string if no any format in the list is supported. If the
              <code>possibleFormats</code> is not given, then returns the most
              suitable image format for this ImageBitmap from all supported
              image formats.
            </p>
            <dl class="parameters">
              <dt>
                optional sequence&lt;ImageFormat&gt; possibleFormats
              </dt>
              <dd>
                A list of image formats that users can handler.
              </dd>
            </dl>
          </dd>
          <dt>
            long mappedDataLength()
          </dt>
          <dd>
            <p>
              Calculate the length of mapped data wile the image is represented
              in the given <code>format</code>.
            </p>
            <p>
              Throws if <code>format</code> is not supported.
            </p>
            <p>
              Return the length (in bytes) of image data that is represented in
              the given <code>format</code>.
            </p>
            <dl class="parameters">
              <dt>
                ImageFormat format
              </dt>
              <dd>
                The format that users want.
              </dd>
            </dl>
          </dd>
          <dt>
            Promise&lt;ImageFormatPixelLayout&gt; mapDataInto()
          </dt>
          <dd>
            <p>
              Makes a copy of the underlying image data in the given format
              <code>format</code> into the given <code>buffer</code> at offset
              <code>offset</code>, filling at most <code>length</code> bytes
              and returns an <a>ImageFormatPixelLayout</a> object which
              describes the pixel layout.
            </p>
            <p>
              Throws if <code>format</code> is not supported.
            </p>
            <p>
              Each time this method is invoked returns a new
              <a>ImageFormatPixelLayout</a> object.
            </p>
            <p>
              Return an <a>ImageFormatPixelLayout</a> object which describes
              the pixel layout.
            </p>
            <dl class="parameters">
              <dt>
                ImageFormat format
              </dt>
              <dd>
                The format that users want.
              </dd>
              <dt>
                BufferSource buffer
              </dt>
              <dd>
                A container for receiving the mapped image data.
              </dd>
              <dt>
                long offset
              </dt>
              <dd>
                The beginning position of the <code>buffer</code> to place the
                mapped data.
              </dd>
              <dt>
                long length
              </dt>
              <dd>
                The length of space in the <code>buffer</code> that could be
                filled.
              </dd>
            </dl>
          </dd>
        </dl>
      </section>
      <section id='imagebitmapfactories-interface-extensions'>
        <h2>
          <code>ImageBitmapFactories</code> interface
        </h2>
        <dl class="idl" title=
        "[NoInterfaceObject, Exposed=(Window,Worker)] partial interface ImageBitmapFactories">
        <dt>
            Promise&lt;ImageBitmap&gt; createImageBitmap()
          </dt>
          <dd>
            <p>
              Create an <code>ImageBitmap</code> from a
              <code>BufferSource</code> containg raw image data.
            </p>
            <dl class="parameters">
              <dt>
                BufferSource buffer
              </dt>
              <dd>
                A container of the raw image data.
              </dd>
              <dt>
                long offset
              </dt>
              <dd>
                The beginning position of the <code>buffer</code> where the raw
                image data is placed.
              </dd>
              <dt>
                long length
              </dt>
              <dd>
                The length of spaces in the <code>buffer</code> that the raw
                image data is palced.
              </dd>
              <dt>
                ImageFormat format
              </dt>
              <dd>
                The format of the raw image data.
              </dd>
              <dt>
                ImageFormatPixelLayout layout
              </dt>
              <dd>
                The pixel layout of the raw image data, which describes how the
                data is arranged in the given <code>buffer</code> as the given
                <code>format</code>.
              </dd>
            </dl>
          </dd>
          <dt>
            Promise&lt;ImageBitmap&gt; createImageBitmap()
          </dt>
          <dd>
            <p>
              Create an <code>ImageBitmap</code> from a
              <code>BufferSource</code> containg raw image data with a given
              cropping area.
            </p>
            <dl class="parameters">
              <dt>
                BufferSource buffer
              </dt>
              <dd>
                A container of the raw image data.
              </dd>
              <dt>
                long offset
              </dt>
              <dd>
                The beginning position of the <code>buffer</code> where the raw
                image data is placed.
              </dd>
              <dt>
                long length
              </dt>
              <dd>
                The length of spaces in the <code>buffer</code> that the raw
                image data is palced.
              </dd>
              <dt>
                ImageFormat format
              </dt>
              <dd>
                The format of the raw image data.
              </dd>
              <dt>
                ImageFormatPixelLayout layout
              </dt>
              <dd>
                The pixel layout of the raw image data, which describes how the
                data is arranged in the given <code>buffer</code> as the given
                <code>format</code>.
              </dd>
              <dt>
                long sx
              </dt>
              <dd>
                The x-coordinate of the starting point of the cropping
                rectangle.
              </dd>
              <dt>
                long sy
              </dt>
              <dd>
                The y-coordinate of the starting point of the cropping
                rectangle.
              </dd>
              <dt>
                long sw
              </dt>
              <dd>
                The width of the cropping rectangle.
              </dd>
              <dt>
                long sh
              </dt>
              <dd>
                The height of the cropping rectangle.
              </dd>
            </dl>
          </dd>
        </dl>
        <p>
          When the <code>createImageBitmap()</code> is invocked, the User Agent
          MUST run the following steps:
        </p>
        <ol>
          <li>If either the <em>sw</em> or <em>sh</em> arguments are specified
          but zero, Return a promise rejected with an IndexSizeError exception
          and abort these steps.
          </li>
          <li>If the <em>buffer</em> has been <code><a href=
          "http://www.w3.org/TR/html51/infrastructure.html#concept-transferable-neutered">
            neutered</a></code>, Return a promise rejected with an
            InvalidStateError exception and abort these steps.
          </li>
          <li>Create a new <code>ImageBitmap</code> object.
          </li>
          <li>Let the <code>ImageBitmap</code> object's bitmap data be the
          image data given by the <code>BufferSource</code> object,
          <code><a href=
          "http://www.w3.org/TR/html51/webappapis.html#cropped-to-the-source-rectangle">
            cropped to the source rectangle</a></code>.
          </li>
          <li>Return a new <code>Promise</code>, but continue running these
          steps <code><a href=
          "http://www.w3.org/TR/html51/infrastructure.html#in-parallel">in
          parallel</a></code>.
          </li>
          <li>
            <code><a href=
            "http://www.w3.org/TR/html51/infrastructure.html#concept-resolver-fulfill">
            Resolve</a></code> the <code>Promise</code> with the new
            <code>ImageBitmap</code> object as the value.
          </li>
        </ol>
      </section>
    </section>
    <section>
      <h2>
        Examples
      </h2>
      <h3>
        WorkerMonitor example:
      </h3>
      <p>
        This example demonstrates how to hook a <code>Worker</code> with a
        <code>MediaStreamTrack</code> and also shows how to use
        <code>postMessage</code> to communicate between main thread and worker
        thread. In this example, the role of control_worker.js is doing flow
        control. This script decide whether drop frame or not. If the system is
        not busy, the script will dispatch current frame to process_worker.js
        or it just drop the frame.
      </p>
      <h4>
        Main file javascript
      </h4>
      <pre class='example highlight'>
        playButton.onclick = function() {
          navigator.getUserMedia({video: true, audio: false} , onSuccess, onFail);
        }
        function onSuccess(stream) {
            localMediaStream = stream;
            initControlWorker();
            if (showWebCamVideo) {
                video.src = window.URL.createObjectURL(localMediaStream);
                video.play();
            }
        }
        function initControlWorker() {
          controlWorker = new Worker("control_worker.js");
          if (!!controlWorker) {
            var tracks = localMediaStream.getTracks();
            controlWorker.postMessage({"type":"set_trackid", "id":tracks[0].id});
            controlWorker.postMessage({"type":"init_process_worker"});
            tracks[0].addVideoMonitor(controlWorker);
          }
          controlWorker.onmessage = function(event) {
            if(event.data.type == "display_arraybuffer") {
              if (!isCanvasInitialized) {
                canvasResult.width = event.data.bitmap.width;
                canvasResult.height = event.data.bitmap.height;
                isCanvasInitialized = true;
              }
              // show result
              ctxResult.drawImage(event.data.bitmap, 0, 0);
            }
          };
        }
      </pre>
      <h4>
        control_worker.js
      </h4>
      <pre class='example highlight'>
        var trackid;
        var worker;
        var isProcessing = false;
        var processWorker;
        onmessage = function(event) {
          if (event.data.type == "init_process_worker") {
            processWorker = new Worker("process_worker.js");

            processWorker.onmessage = function(event) {
              if (event.data.type == "display_arraybuffer") {
                // send back to the main thread
                postMessage({"type":event.data.type,
                             "bitmap":event.data.bitmap});
                isProcessing = false;
              }
            };
          } else if (event.data.type == "set_trackid") {
            trackid = event.data.id;
          }
        }

        var frameNum = 0;
        var dropNum = 0;

        function showDropRate() {
          console.log("Drop rate = " + 100 * dropNum / frameNum + "%");
        }

        onvideoprocess = function(event) {
          if (frameNum % 100 == 1) {
            showDropRate();
          }

          frameNum++;
          if (isProcessing) {
            // drop this frame
            console.log("drop frame[" + frameNum + "]");
            dropNum++;
            return;
          } else {
            // console.log("process frame[" + frameNum + "]")
            isProcessing = true;
            // send to the process worker
            processWorker.postMessage({"type":"convert_color",
                                       "bitmap":event.inputImageBitmap});
          }
        };
      </pre>
      <h4>
        process_worker.js
      </h4>
      <pre class='example highlight'>
        function processOneFrame_RGBA(bitmap) {
          // Do what you want to do!
          ...
        }
        onmessage = function(event) {
          // do the invernt effect
          processOneFrame_RGBA(event.data.bitmap);

          // send back to the control worker
          postMessage({"type":"display_arraybuffer",
                       "bitmap":event.data.bitmap});
        };
      </pre>
      <h3>
        WorkerMonitor example in multiple workers:
      </h3>
      <p>
        This example is extended from previous "WorkerMonitor example". The
        main difference is this example create a WorkerPool object in
        control_worker.js. The script will reuse the workers in the pool. This
        mechanism full utilized the power of multi-core machine to reduce the
        frame drop rate.
      </p>
      <h4>
        Main file javascript
      </h4>
      <pre class='example highlight'>
        playButton.onclick = function() {
            navigator.getUserMedia({video: true, audio: false} , onSuccess, onFail);
        }
        stopButton.onclick = function() {
            isContinuous = false;
            video.pause();
            localMediaStream.stop();
        }
        function onSuccess(stream) {
            localMediaStream = stream;
            initControlWorker();
            if (showWebCamVideo) {
                video.src = window.URL.createObjectURL(localMediaStream);
                video.play();
            }
        }
        function onFail(e) {
            console.log('Cannot access WebCAM!', e);
        }
        function initControlWorker() {
          controlWorker = new Worker("control_worker.js");
          if (!!controlWorker) {
            var tracks = localMediaStream.getTracks();
            controlWorker.postMessage({"type":"set_trackid", "id":tracks[0].id});
            controlWorker.postMessage({"type":"init_process_worker"});
            tracks[0].addVideoMonitor(controlWorker);
          }
          controlWorker.onmessage = function(event) {
            if (event.data.taskType == "DisplayTask") {
              if (!isCanvasInitialized) {
                canvasResult.width = event.data.bitmap.width;
                canvasResult.height = event.data.bitmap.height;
                isCanvasInitialized = true;
              }
              ctxResult.drawImage(event.data.bitmap, 0, 0);
            }
          };
        }
      </pre>
      <h4>
        control_worker.js
      </h4>
      <pre class='example highlight'>
        /*
         * WorkerPool
         */
        function WorkerPool() {
          this.avaialableQueue = [];
          this.workingQueue = [];
        }

        WorkerPool.prototype.Init = function(script, numOfWorkers) {
          for (var i = 0; i &lt; numOfWorkers; ++i) {
            this.avaialableQueue.push(new ProcessWorker(i, script, this));
          }
        }

        WorkerPool.prototype.Process = function(frameNum, bitmap) {
          var task = new ProcessTask(frameNum, bitmap);
          return this.Dispatch(task);
        }

        WorkerPool.prototype.Dispatch = function(task) {
          if (task.taskType == ProcessTask.TASK_TYPE) {
            if (this.avaialableQueue.length &gt; 0) {
              var processWorker = this.avaialableQueue.shift();
              processWorker.worker.postMessage(task);
              this.workingQueue.push(processWorker);
              return true;
            }
            else {
              console.log("No availabe ProcessWorker now, drop this frame[" + task.frameNum + "]......");
              return false;
            }
          } else {
            console.log("Cannot handle " + task.taskType);
            return false;
          }
        }

        WorkerPool.prototype.Display = function() {
          while(this.workingQueue.length &gt; 0) {
            if (!!(this.workingQueue[0].displayTask)) {
              var processWorker = this.workingQueue.shift();
              postMessage(processWorker.displayTask);
              processWorker.displayTask = null;
              this.avaialableQueue.push(processWorker);
            } else {
              break;
            }
          }
        }

        WorkerPool.prototype.FindProcessWorker = function(worker) {
          for (var i = 0; i &lt; this.workingQueue.length; ++i) {
            if (this.workingQueue[i].worker == worker) {
              return this.workingQueue[i];
            }
          }
        }

        /*
         * ProcessWorker
         */
        function ProcessWorker(id, script, pool) {
          this.workerID = id;
          this.workerPool = pool
          this.displayTask = null;
          this.worker = new Worker(script);
          this.worker.wraper = this;
          this.worker.onmessage = function(event) {
            var processWorker = workerPool.FindProcessWorker(this);
            processWorker.displayTask = new DisplayTask(event.data.frameNum, event.data.bitmap);
            workerPool.Display();
            // workerPool.PrintInfo("in worker.onmessage()");
          }
        }

        /*
         * Task, ProcessTask, DisplayTask
         */
        function Task(type, frameNum) {
          this.taskType = type;
          this.frameNum = frameNum;
        }

        function ProcessTask(frameNum, bitmap) {
          Task.call(this, ProcessTask.TASK_TYPE, frameNum);
          this.bitmap = bitmap;
        }
        ProcessTask.prototype = Object.create(Task.prototype);
        ProcessTask.prototype.constructor = ProcessTask;
        ProcessTask.TASK_TYPE = "ProcessTask";

        function DisplayTask(frameNum, bitmap) {
          Task.call(this, DisplayTask.TASK_TYPE, frameNum);
          this.bitmap = bitmap;
        }
        DisplayTask.prototype = Object.create(Task.prototype);
        DisplayTask.prototype.constructor = DisplayTask;
        DisplayTask.TASK_TYPE = "DisplayTask";
        var numOfWorkers = 2; // initialize the WorkerPool whit this number of workers.
        var workerPool;
        var trackid;
        var frameNum = 0;
        var dropNum = 0;

        function showDropRate() {
          console.log("Drop rate = " + 100 * dropNum / frameNum + "%");
        }


        onmessage = function(event) {
          if (event.data.type == "init_process_worker") {
            workerPool = new WorkerPool();
            workerPool.Init("process_worker.js", numOfWorkers);
          } else if (event.data.type == "set_trackid") {
            trackid = event.data.id;
          }
        }

        onvideoprocess = function(event) {
          if (frameNum % 100 == 1) {
            showDropRate();
          }

          if (!workerPool.Process(frameNum++, event.inputImageBitmap)) {
            dropNum++;
          }
        };
      </pre>
      <h4>
        process_worker.js
      </h4>
      <pre class='example highlight'>
        function processOneFrame_RGBA(bitmap) {
          // Do what you want to do!
          ...
        }
        onmessage = function(event) {
          // do the invernt effect
          processOneFrame_RGBA(event.data.bitmap);

          // send back to the control worker
          postMessage({"type":"display_arraybuffer",
               "frameNum":event.data.frameNum,
               "bitmap":event.data.bitmap});
        };
      </pre>
      <h3>
        WorkerProcessor example:
      </h3>
      <p>
        This example shows how to process and display the processed
        <a>MediaStreamTrack</a>. Be careful, the <code>Worker</code> is
        appended into the new <a>MediaStreamTrack</a>, not original one.
      </p>
      <h4>
        Main file javascript
      </h4>
      <pre class='example highlight'>
        playButton.onclick = function() {
            navigator.getUserMedia({video: true, audio: false} , onSuccess, onFail);
        }
        stopButton.onclick = function() {
            isContinuous = false;
            video.pause();
            localMediaStream.stop();
        }
        function onSuccess(stream) {
            localMediaStream = stream;
            initControlWorker();
            if (showWebCamVideo) {
                video.src = window.URL.createObjectURL(localMediaStream);
                video.play();
            }
        }
        function onFail(e) {
            console.log('Cannot access WebCAM!', e);
        }
        function initControlWorker() {
          controlWorker = new Worker("processor.js");
          if (!!controlWorker) {
            var tracks = localMediaStream.getTracks();
            controlWorker.postMessage({"type":"set_trackid", "id":tracks[0].id});
            var newtrack = tracks[0].addVideoProcessor(controlWorker);
            // link to the result video
            resultVideo.mozSrcObject = new MediaStream([newtrack]);
            resultVideo.play();
          }
          controlWorker.onmessage = function(event) {
            if(event.data.type == "display_arraybuffer") {
              if (!isCanvasInitialized) {
                resultVideo.width = event.data.imageWidth;
                resultVideo.height = event.data.imageHeight;
                isCanvasInitialized = true;
              }
            }
          };
        }
      </pre>
      <h4>
        processor.js
      </h4>
      <pre class='example highlight'>
        function processOneFrame_Invert(inputBitmap, outputBitmap) {
          var bitmap = inputBitmap;
          var format = bitmap.findOptimalFormat();
          format = "RGBA32"; // force it to be RGBA32, do conversion in Gecko
          var length = bitmap.mappedDataLength(format);

          if (format != bitmapFormat || length != bitmapBufferLength) {
            bitmapFormat = format;
            bitmapBufferLength = length;
            bitmapBuffer = new ArrayBuffer(bitmapBufferLength);
            bitmapBufferView = new Uint8ClampedArray(bitmapBuffer, 0, bitmapBufferLength);
          }
          var bitmapPixelLayout = bitmap.mapDataInto(bitmapFormat, bitmapBuffer, 0, bitmapBufferLength);

          if (!isInitialized) {
            rgbaBufferLength = bitmapBufferLength;
            rgbaBuffer = new ArrayBuffer(rgbaBufferLength);
            rgbaBufferView = new Uint8ClampedArray(rgbaBuffer, 0, rgbaBufferLength);

            isInitialized = true;
          }

          // convert YUV to Gray or RGBA
          for (var i = 0; i &lt; bitmap.height; ++i) {
            for (var j = 0; j &lt; bitmap.width; ++j) {

              /*
               *  do invert effect
               */
              var index = bitmap.width * i + j;
              rgbaBufferView[index * 4 + 0] = 255 - bitmapBufferView[index * 4 + 0];
              rgbaBufferView[index * 4 + 1] = 255 - bitmapBufferView[index * 4 + 1];
              rgbaBufferView[index * 4 + 2] = 255 - bitmapBufferView[index * 4 + 2];
              rgbaBufferView[index * 4 + 3] = bitmapBufferView[index * 4 + 3]; // no change in the appha channel
            }
          }

          // write back to event outputImageBitmap
          outputBitmap.setDataFrom("RGBA32", rgbaBuffer, 0, rgbaBufferLength,
                                   bitmap.width, bitmap.height, bitmapPixelLayout.channels[0].stride);
        }
        onvideoprocess = function(event) {
          processOneFrame_Invert(event.inputImageBitmap, event.outputImageBitmap);
        }
      </pre>
    </section>
    <section class='appendix'>
      <h2>
        Acknowledgements
      </h2>
      <p>
        Thanks to Robert O'Callahan for his idea of this design.
      </p>
    </section>
  </body>
</html>

<!DOCTYPE html>
<html>
  <head>
    <title>
      Media Capture Stream with Worker
    </title>
    <meta charset='utf-8'>
    <script src='https://www.w3.org/Tools/respec/respec-w3c-common' async
    class='remove'>
    </script>
    <script class='remove'>
      var respecConfig = {
          // specification status (e.g. WD, LCWD, WG-NOTE, etc.). If in doubt use ED.
          specStatus:           "ED",

          // the specification's short name, as in http://www.w3.org/TR/short-name/
          shortName:            "mediacapture-worker",

          // if your specification has a subtitle that goes below the main
          // formal title, define it here
          // subtitle   :  "an excellent document",

          // if you wish the publication date to be other than the last modification, set this
          // publishDate:  "2009-08-06",

          // if the specification's copyright date is a range of years, specify
          // the start date here:
          // copyrightStart: "2005"

          // if there is a previously published draft, uncomment this and set its YYYY-MM-DD date
          // and its maturity status
          // previousPublishDate:  "1977-03-15",
          // previousMaturity:  "WD",

          // if there a publicly available Editor's Draft, this is the link
           edDraftURI:           "https://w3c.github.io/mediacapture-worker/",

          // if this is a LCWD, uncomment and set the end of its review period
          // lcEnd: "2009-08-05",

          // editors, add as many as you like
          // only "name" is required
          editors:  [
              {
                  name:       "Chia-hung Tai"
    //              ,   url:        "http://example.org/"
              ,   mailto:     "ctai@mozilla.com"
              ,   company:    "Mozilla"
              ,   companyURL: "https://www.mozilla.org/en-US/foundation/moco/"
              },
              {
                  name:       "Robert O'Callahan"
    //              ,   url:        "http://example.org/"
              ,   company:    "Mozilla"
              ,   companyURL: "https://www.mozilla.org/en-US/foundation/moco/"
              },
              {
                  name:       "Tzuhao Kuo"
    //              ,   url:        "http://example.org/"
              ,   mailto:     "tkuo@mozilla.com"
              ,   company:    "Mozilla"
              ,   companyURL: "https://www.mozilla.org/en-US/foundation/moco/"
              },
          ],

          // name of the WG
          wg: [
                        "Web Real-Time Communication Working Group",
                        "Device APIs Working Group"
          ],
          // URI of the public WG page
          wgURI: [      "http://www.w3.org/2011/04/webrtc/",
                        "http://www.w3.org/2009/dap/"
          ],
          // name (without the @w3c.org) of the public mailing to which comments are due
          wgPublicList: "public-media-capture",

          // URI of the patent status for this WG, for Rec-track documents
          // !!!! IMPORTANT !!!!
          // This is important for Rec-track documents, do not copy a patent URI from a random
          // document unless you know what you're doing. If in doubt ask your friendly neighbourhood
          // Team Contact.
          wgPatentURI:  ["", ""],
          // !!!! IMPORTANT !!!! MAKE THE ABOVE BLINK IN YOUR HEAD
      };
    </script>
  </head>
  <body>
    <section id='abstract'>
      <p>
        This specification extends the <em>Media Capture and Streams</em>
        specification [[!GETUSERMEDIA]] to allow JavaScript developers to
        process video frame data in workers on the web applications.
      </p>
    </section>
    <section id='sotd'>
      <p>
        <strong>This document is not complete and is subject to change. Early
        experimentations are encouraged to allow the Media Capture Task Force
        to evolve the specification based on technical discussions within the
        Task Force, implementation experience gained from early
        implementations, and feedback from other groups and
        individuals.</strong>
      </p>
      <p>
        <strong>This specification is started in the Media Capture Task Force
        to get it under way in anticipation of the <a href=
        "http://www.w3.org/2015/07/timed-media-wg.html">Timed Media Working
        Group</a> creation. The intention is to transfer this specification to
        the Timed Media Working Group when the group is formed.</strong>
      </p>
      <p>
        <strong>This specification will use the <a href=
        "http://www.w3.org/Consortium/Legal/2015/copyright-software-and-document.html">
        W3C Software and Document license</a> to be consistent with the
        <a href="http://www.w3.org/2015/07/timed-media-wg.html#licensing">licensing
        terms</a> of the proposed Timed Media Working Group.</strong>
      </p>
    </section>
    <section>
      <h2>
        Introduction
      </h2>
      <p>
        The <em>Media Capture and Streams</em> specification provides a way to
        access to the video camera. But how to process the video frame data in
        JavaScript is not covered. By associating a worker with
        <a><code>MediaStreamTrack</code></a>s
        (<code>MediaStreamTrack.kind</code> must be "video"), the framework can
        dispatch a
        <a><code>VideoMonitorEvent</code></a>/<a><code>VideoProcessorEvent</code></a>
        to the worker frame by frame. Then JavaScript developer can write a
        video processing script which enables processing, analyzing of video
        data directly using JavaScript in a Worker thread.
      </p>
      <p>
        <img alt="The relationship between Worker and MediaStreamTrack" src=
        "images/Worker%20-%20FLOW.png" style="width:60%">
      </p>
      <p>
        The design principle of this specification is to provide a push-like
        mechanism for video processing. This design gives the Web developers no
        worry about any platform capability to full utilize the CPU usage. The
        Web developer no longer need to decide the FPS rate to grab the input
        video frame. The User Agent only dispatch the
        <a><code>VideoMonitorEvent</code></a>/<a><code>VideoProcessorEvent</code></a>
        when a new video frame arriving.
      </p>
    </section>
    <section>
      <h2>
        Use cases and requirements
      </h2>
      <p>
        This specification attempts to address the <a href=
        "https://wiki.mozilla.org/Project_FoxEye#Use_Cases">Use Cases and
        Requirements</a> for expanding the Web potentials to image processing
        and computer vision area.
      </p>
    </section>
    <section>
      <h2>
        Conformance
      </h2>
      <p>
        This specification defines conformance criteria that apply to a single
        product: the <dfn>user agent</dfn> that implements the interfaces that
        it contains.
      </p>
      <p>
        Implementations that use ECMAScript to implement the APIs defined in
        this specification must implement them in a manner consistent with the
        ECMAScript Bindings defined in the Web IDL specification [[!WEBIDL]],
        as this specification uses that specification and terminology.
      </p>
      <p>
        The key words "MUST", "MUST NOT", "REQUIRED", "SHOULD", "SHOULD NOT",
        "RECOMMENDED", "MAY", and "OPTIONAL" in the normative parts of this
        document are to be interpreted as described in RFC2119. For
        readability, these words do not appear in all uppercase letters in this
        specification. [[!RFC2119]]
      </p>
    </section>
    <section>
      <h2>
        Dependencies
      </h2>
      <p>
        The <code><a href=
        "http://w3c.github.io/mediacapture-main/getusermedia.html#idl-def-MediaStreamTrack">
        <dfn>MediaStreamTrack</dfn></a></code> and <code><a href=
        "http://w3c.github.io/mediacapture-main/getusermedia.html#idl-def-MediaStream">
        <dfn>MediaStream</dfn></a></code> interfaces this specification extends
        are defined in [[!GETUSERMEDIA]].
      </p>
      <p>
        The <code><a href=
        "http://www.w3.org/TR/html51/webappapis.html#imagebitmap"><dfn>ImageBitmap</dfn></a></code>
        interface and <code><a href=
        "http://www.w3.org/TR/html51/webappapis.html#imagebitmapfactories"><dfn>
        ImageBitmapFactories</dfn></a></code> interface this specification
        extends are defined in [[!HTML51]].
      </p>
      <p>
        The <code><a href=
        "http://www.w3.org/TR/WebIDL-1/#common-BufferSource"><dfn>BufferSource</dfn></a></code>
        is defined in [[!WEBIDL]]
      </p>
      <p>
        The following concepts and interfaces are defined in [[!HTML]]:
      </p>
      <ul>
        <li>
          <dfn><a href=
          "https://html.spec.whatwg.org/multipage/webappapis.html#event-handler-idl-attributes">
          event handler IDL attribute</a></dfn>
        </li>
      </ul>
    </section>
    <section>
      <h2>
        Extensions
      </h2>
      <section>
        <h2>
          <code>DedicatedWorkerGlobalScope</code> interface
        </h2>
        <p link-for="DedicatedWorkerGlobalScope">
          To enable generating, processing, and analyzing of video data
          directly using JavaScript in a worker, the
          <a>DedicatedWorkerGlobalScope</a> interface is extended with a
          <a>onvideoprocess</a> <a>event handler IDL attribute</a>. <a>Video
          monitor events</a> and <a>video processing events</a> are dispatched
          at the <a>DedicatedWorkerGlobalScope</a> object.
        </p>
        <pre class="idl">
          partial interface DedicatedWorkerGlobalScope {
                          attribute EventHandler onvideoprocess;
          };
        </pre>
        <p dfn-for="DedicatedWorkerGlobalScope">
          The <dfn><code>onvideoprocess</code></dfn> is an <a>event handler IDL
          attribute</a> for the <dfn><code>videoprocess</code></dfn> event
          type.
        </p>
      </section>
      <section>
        <h2>
          <code id="event-videomonitorevent">VideoMonitorEvent</code> interface
        </h2>
        <p>
          <dfn>Video monitor events</dfn> use the <a>VideoMonitorEvent</a>
          interface for their <a>videoprocess</a> events:
        </p>
        <pre class="idl">
          [Constructor(DOMString type, optional VideoMonitorEventInit videoMonitorEventInitDict), Exposed=Worker]
          interface VideoMonitorEvent : Event {
              readonly    attribute DOMString   trackId;
              readonly    attribute double      playbackTime;
              readonly    attribute ImageBitmap inputImageBitmap;
          };

          dictionary VideoMonitorEventInit : EventInit {
            DOMString   trackId;
            double      playbackTime;
            ImageBitmap inputImageBitmap;
          };
        </pre>
        <div dfn-for="VideoMonitorEvent">
          <p>
            The <dfn>trackId</dfn> attribute must return the value it was
            initialized to. When the object is created, this attribute must be
            initialized to the empty string. It represents the identifier that
            is shared with the <a>video worker</a> and its <a>video worker
            source</a>.
          </p>
          <p>
            <!-- TODO: is this current stream position? -->
             The <dfn>playbackTime</dfn> attribute must return the value it was
            initialized to. When the object is created, this attribute must be
            initialized to zero. It represents the current stream position, in
            seconds.
          </p>
          <p>
            The <dfn>inputImageBitmap</dfn> attribute must return the value it
            was initialized to. When the object is created, this attribute must
            be initialized to null. It represents the <a>ImageBitmap</a> object
            whose bitmap data is provided by the <a>video worker source</a>.
          </p>
          <p>
            A worker is said to be a <dfn>video worker</dfn> if it has a
            <a>video worker source</a>.
          </p>
          <p>
            A <a>MediaStreamTrack</a> object is said to be a <dfn>video worker
            source</dfn> for a worker <var>w</var>, if its
            <code>addVideoMonitor()</code> method has been invoked with
            <var>w</var> as an argument.
          </p>
          <p>
            When a user agent is required to <dfn>fire a video monitor
            event</dfn>, it must run the following steps:
          </p>
        </div>
        <ol link-for="VideoMonitorEvent">
          <li>Create a <a>VideoMonitorEvent</a>, initialize it to have a name
          <a>videoprocess</a>, to not bubble, to not be cancelable.
          </li>
          <li>Initialize the <a>trackId</a> attribute to the value of the
          <code>id</code> attribute of the <a>video worker source</a>.
          </li><!-- TODO -->
          <li>Initialize the <a>playbackTime</a> attribute to the value of the
          <code>currentTime</code> attribute of the <a>MediaStream</a> that
          contains the <a>video worker source</a>.
          </li>
          <li>Initialize the <a>inputImageBitmap</a> attribute to the bitmap
          data of the <a>video worker source</a>'s video frame at the current
          stream position at <a>playbackTime</a>.
          </li>
          <li>Dispatch the newly created <a>VideoMonitorEvent</a> object at the
          <a>DedicatedWorkerGlobalScope</a> of the <a title=
          "video worker">video workers</a> that share the same <a>video worker
          source</a>.
          </li>
        </ol>
      </section>
      <section>
        <h2>
          <code id="event-videoprocessorevent">VideoProcessorEvent</code>
          interface
        </h2>
        <p>
          This event is inherited from <code><a>VideoMonitorEvent</a></code>.
          When the <code>MediaStreamTrack</code> comes a new video frame data,
          the User Agent will dispatch a <code>VideoProcessorEvent</code> to
          the associated Workers when the association is connected by
          <code>addVideoProcessor</code>. The event handlers of those Workers
          process video from the input by accessing the video data from the
          <code>inputImageBitmap</code> attribute in the
          <code>VideoProcessorEvent</code>. The processed video data which is
          the result of the processing is then placed into the
          <code>outputImageBitmap</code>. The User Agent will append the
          <code>outputImageBitmap</code> into the new created
          <code>MediaStreamTrack</code>.
        </p>
        <p>
          Ideally the <code>MediaStreamTrack</code> should dispatch each video
          frame through <a>VideoProcessorEvent</a>. But sometimes the worker
          thread could not process the frame in time. So the implementation
          could skip the frame to avoid high memory footprint. In such case, we
          might not be able to process every frame in a real time
          <code>MediaStream</code>.
        </p>
        <dl class="idl" title=
        "[Exposed=Worker] interface VideoProcessorEvent : VideoMonitorEvent">
          <dt>
            attribute ImageBitmap? outputImageBitmap
          </dt>
          <dd>
            The output video frame comes to the corresponding
            <code>MediaStreamTrack</code>. It is default to be null. The Web
            developer need to create a new <code>ImageBitmap</code> for output
            frame and assign it to <code>outputImageBitmap</code>.
          </dd>
        </dl>
      </section>
      <section>
        <h2>
          <code>MediaStreamTrack</code> interface
        </h2>
        <dl class="idl" title="partial interface MediaStreamTrack">
          <dt>
            void addVideoMonitor(Worker worker)
          </dt>
          <dd>
            Associate the <code>worker</code> with the
            <code>MediaStreamTrack</code>. You can add the same
            <code>worker</code> to any other <code>MediaStreamTrack</code>.
          </dd>
          <dt>
            void removeVideoMonitor(Worker worker)
          </dt>
          <dd>
            Remove a particular <code>Worker</code> from the
            <code>MediaStreamTrack</code>. User Agent should throw exception
            when the <code>Worker</code> is not exist in the
            <code>MediaStreamTrack</code>.
          </dd>
          <dt>
            MediaStreamTrack addVideoProcessor(Worker worker)
          </dt>
          <dd>
            This method will create a new <code>MediaStreamTrack</code> and
            take the original <code>MediaStreamTrack</code> as the input
            source. Then associate the <code>Worker</code> with new created
            <code>MediaStreamTrack</code>. The developers can build a processed
            pipeline by this method.
          </dd>
          <dt>
            void removeVideoProcessor()
          </dt>
          <dd>
            Remove the added <code>Worker</code> from current
            <code>MediaStreamTrack</code>. User Agent should throw exception
            when the <code>Worker</code> is not exist in the
            <code>MediaStreamTrack</code>. A <code>MediaStreamTrack</code> owns
            at most one Worker processor in any time.
          </dd>
        </dl>
      </section>
    </section>
    <section id='imagebitmap-extensions'>
      <h2>
        ImageBitmap extensions
      </h2>
      <div class="note">
        <p>
          The <a>ImageBitmap</a> interface is originally designed as a pure
          opaque handler to an image data buffer inside a browser so that how
          the browser stores the buffer is uknown to users and optimized to
          platforms. In this specification, we chooses <a>ImageBitmap</a>
          (instead of <code>ImageData</code>) as the container of video frames
          because the decoded video frame data might exist in either CPU or GPU
          memory which perfectly matches the nature of
          <a><code>ImageBitmap</code></a> as an opaque handler.
        </p>
      </div>
      <p>
        Considering how would developers process video frames, there are two
        possible approaches, via pure JavaScript(/asm.js) code or via WebGL.
      </p>
      <ol>
        <li>If developers use JavaScript(/asm.js) to process the frames, then
        the <a>ImageBitmap</a> interface needs to be extended with APIs for
        developers to access its underlying data and there should also be a way
        for developers to create an <a>ImageBitmap</a> from the processed data.
        </li>
        <li>If developers use WebGL, then WebGL needs to be extended so that
        developers can pass an <a>ImageBitmap</a> into the WebGL context and
        the browser will handle how to upload the raw image data into the GPU
        memory. Possiblely, the data is already in the GPU memory so that the
        operation could be very efficient.
        </li>
      </ol>
      <p>
        In this specification, the original <a>ImageBitmap</a> interface is
        extended with three methods to let developers read data from an
        <a>ImageBitmap</a> object into a given <a>BufferSource</a> in a set of
        supported <a>ImageFormat</a>s and two interfaces,
        <a>ImageFormatPixelLayout</a> and <a>ChannelPixelLayout</a>, are
        proposed to work with the extend <a>ImageBitmap</a> methods to describe
        how the accessed image data is arranged in memory. Also, the
        <a>ImageBitmapFactories</a> interface is extended to let developers
        create an <a>ImageBitmap</a> object from a given <a>BufferSource</a>.
      </p>
      <section id='imageformat'>
        <h2>
          ImageFormat
        </h2>
        <p>
          An image or a video frame is conceptually a two-dimentional array of
          data and each element in the array is called a <dfn>pixel</dfn>.
          However, the pixels are usually stored in a one-dimentional array and
          could be arranged in a variety of <a>ImageFormat</a>s. Developers
          need to know how the pixels are formatted so that they are able to
          process it. An <a>ImageFormat</a> describes how pixels in an image
          are arranged and all pixels in one single image are arranged in the
          same way. A single pixel has at least one, but usually multiple
          <dfn>pixel value</dfn>s. The range of a pixel value varies, which
          means different <a>ImageFormat</a>s use different <dfn>data
          type</dfn>s to store a single pixel value. The most frequently used
          data type is 8-bit unsigned interger whose range is from 0 to 255,
          others could be 16-bit interger or 32-bit folating points and so
          forth. The number of pixle values of a single pixel is called the
          number of <dfn>channel</dfn>s of the <a>ImageFormat</a>. Multiple
          pixel valuse of a pixel are used together to describe the captured
          property which could be color or depth information. For example, if
          the data is a color image in RGB color space, then it is a
          three-channel <a>ImageFormat</a> and a pixel is described by R, G and
          B three pixel values with range from 0 to 255. Another example, if
          the data is a gray image, then it is a single-channel
          <a>ImageFormat</a> with 8-bit unsigned interger data type and the
          pixel value describes the gray scale. For depth data, it is a single
          channel <a>ImageFormat</a> too, but the data type is 16-bit unsigned
          interger and the pixel value is the depth level. For those
          <a>ImageFormat</a>s whose pixels contain multiple pixel values, the
          pixel values might be arranged in a planar way or interleaving way:
        </p>
        <ol>
          <li>
            <dfn>Planar pixel layout</dfn>: each channel has its pixel values
            stored consecutively in separated buffers (a.k.a. planes) and then
            all channel buffers are stored consecutively in memory. (Ex:
            RRRRRR......GGGGGG......BBBBBB......)
          </li>
          <li>
            <dfn>Interleaving pixel layout</dfn>: each pixel has its pixel
            values from all channels stored together and interleaves all
            channels. (Ex: RGBRGBRGBRGBRGB......)
          </li>
        </ol>
        <div class="note">
          <p>
            <a>ImageFormat</a>s belong to the same color space might have
            different pixel layouts.
          </p>
        </div>
        <section id='imageformat-enumaration'>
          <h2>
            <code>ImageFormat</code> enumeration
          </h2>
          <p>
            An enumeration <a>ImageFormat</a> defines a list of image formats
            which are supported by the browser and exposed to users. The extend
            APIs in this specification use this enumeration to negotiate the
            format while accessing the underlying data of <a>ImageBitmap</a>
            and creating a new <a>ImageBitmap</a>.
          </p>
          <div class="note">
            <p>
              We need to elaborate this list for standardization.
            </p>
          </div>
          <dl id="enum-basic" class="idl" title="enum ImageFormat">
            <dt>
              RGBA32
            </dt>
            <dd>
              <p>
                Channel order: R, G, B, A
              </p>
              <p>
                Channel size: full rgba-chennels
              </p>
              <p>
                Pixel layout: interleaving rgba-channels
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              BGRA32
            </dt>
            <dd>
              <p>
                Channel order: B, G, R, A
              </p>
              <p>
                Channel size: full bgra-channels
              </p>
              <p>
                Pixel layout: interleaving bgra-channels
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              RGB24
            </dt>
            <dd>
              <p>
                Channel order: R, G, B
              </p>
              <p>
                Channel size: full rgb-channels
              </p>
              <p>
                Pixel layout: interleaving rgb-channels
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              BGR24
            </dt>
            <dd>
              <p>
                Channel order: B, G, R
              </p>
              <p>
                Channel size: full bgr-channels
              </p>
              <p>
                Pixel layout: interleaving bgr-channels
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              GRAY8
            </dt>
            <dd>
              <p>
                Channel order: GRAY
              </p>
              <p>
                Channel size: full gray-channel
              </p>
              <p>
                Pixel layout: planar gray-channel
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              YUV444P
            </dt>
            <dd>
              <p>
                Channel order: Y, U, V
              </p>
              <p>
                Channel size: full yuv-channels
              </p>
              <p>
                Pixel layout: planar yuv-channels
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              YUV422P
            </dt>
            <dd>
              <p>
                Channel order: Y, U, V
              </p>
              <p>
                Channel size: full y-channel, half uv-channels
              </p>
              <p>
                Pixel layout: planar yuv-channels
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              YUV420P
            </dt>
            <dd>
              <p>
                Channel order: Y, U, V
              </p>
              <p>
                Channel size: full y-channel, quarter uv-channels
              </p>
              <p>
                Pixel layout: planar yuv-channels
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              YUV420SP_NV12
            </dt>
            <dd>
              <p>
                Channel order: Y, U, V
              </p>
              <p>
                Channel size: full y-channel, quarter uv-channels
              </p>
              <p>
                Pixel layout: planar y-channel, interleaving uv-channels
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              YUV420SP_NV21
            </dt>
            <dd>
              <p>
                Channel order: Y, V, U
              </p>
              <p>
                Channel size: full y-channel, quarter uv-channels
              </p>
              <p>
                Pixel layout: planar y-channel, interleaving vu-channels
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              HSV
            </dt>
            <dd>
              <p>
                Channel order: H, S, V
              </p>
              <p>
                Channel size: full hsv-channels
              </p>
              <p>
                Pixel layout: interleaving hsv-channels
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              Lab
            </dt>
            <dd>
              <p>
                Channel order: l, a, b
              </p>
              <p>
                Channel size: full lab-channels
              </p>
              <p>
                Pixel layout: interleaving lab-channels
              </p>
              <p>
                Data type: 8-bit unsigned integer
              </p>
            </dd>
            <dt>
              DEPTH
            </dt>
            <dd>
              <p>
                Channel order: DEPTH
              </p>
              <p>
                Channel size: full depth-channel
              </p>
              <p>
                Pixel layout: planar depth-channel
              </p>
              <p>
                Data type: 16-bit unsigned integer
              </p>
            </dd>
          </dl>
        </section>
        <section id='datatype-enumeration'>
          <h2>
            <code>DataType</code> enumeration
          </h2>
          <p>
            An enumeration <a>DataType</a> defines a list of data types that is
            used to store a single <a>pixel value</a>.
          </p>
          <dl id="enum-basic" class="idl" title="enum DataType">
            <dt>
              uint8
            </dt>
            <dd>
              8-bit unsigned integer.
            </dd>
            <dt>
              int8
            </dt>
            <dd>
              8-bit integer.
            </dd>
            <dt>
              uint16
            </dt>
            <dd>
              16-bit unsigned integer.
            </dd>
            <dt>
              int16
            </dt>
            <dd>
              16-bit integer.
            </dd>
            <dt>
              uint32
            </dt>
            <dd>
              32-bit unsigned integer.
            </dd>
            <dt>
              int32
            </dt>
            <dd>
              32-bit integer.
            </dd>
            <dt>
              float32
            </dt>
            <dd>
              32-bit IEEE floating point number.
            </dd>
            <dt>
              float64
            </dt>
            <dd>
              64-bit IEEE floating point number.
            </dd>
          </dl>
        </section>
      </section>
      <section id='pixellayout'>
        <h2>
          PixelLayout
        </h2>
        <p>
          Two interfaces, <a>ImageFormatPixelLayout</a> and
          <a>ChannelPixelLayout</a>, help together to generalize the variety of
          pixel layouts among image formats.
        </p>
        <p>
          The <a>ImageFormatPixelLayout</a> represents the pixel layout of a
          certain image format and since a image format is composed by at least
          one <a>channel</a> so an <a>ImageFormatPixelLayout</a> object
          contains at least one <a>ChannelPixelLayout</a> object.
        </p>
        <p>
          Although an image or a video frame is a two-dimensional structure,
          its data is usually stored in an one-dimensional array in the
          raw-major way and the <a>ChannelPixelLayout</a> uses the following
          properties to describe how pixel values are arranged in the one
          dimentional array buffer.
        </p>
        <ol>
          <li>
            <strong>offset</strong>: where is each <a>channel</a>'s data starts
            from. (Relative to the beginning of the video data one-dimension
            array.)
          </li>
          <li>
            <strong>width</strong> and <strong>height</strong>: how much
            samples are in each channel.
          </li>
          <li>
            <strong>data type</strong>: the data type used to store one single
            <a>pixel value</a>.
          </li>
          <li>
            <strong>stride</strong>: the total bytes of each raw plus the
            padding bytes of each row.
          </li>
          <li>
            <strong>skip</strong>: this is used to describe <a>interleaving
            pixel layout</a>. (For <a>planar pixel layout</a>, this property
            will be zero.)
          </li>
        </ol>
        <pre class='example highlight'>
          Example1: RGBA image, width = 620, height = 480, stride = 2560

          chanel_r: offset = 0, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3
          chanel_g: offset = 1, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3
          chanel_b: offset = 2, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3
          chanel_a: offset = 3, width = 620, height = 480, data type = uint8, stride = 2560, skip = 3

                  &lt;---------------------------- stride ----------------------------&gt;
                  &lt;---------------------- width x 4 ----------------------&gt;
          [index] 01234   8   12  16  20  24  28                           2479    2559
                  |||||---|---|---|---|---|---|----------------------------|-------|
          [data]  RGBARGBARGBARGBARGBAR___R___R...                         A%%%%%%%%
          [data]  RGBARGBARGBARGBARGBAR___R___R...                         A%%%%%%%%
          [data]  RGBARGBARGBARGBARGBAR___R___R...                         A%%%%%%%%
                       ^^^
                       r-skip
        </pre>
        <pre class='example highlight'>
          Example2: YUV420P image, width = 620, height = 480, stride = 640

          chanel_y: offset = 0, width = 620, height = 480, stride = 640, skip = 0
          chanel_u: offset = 307200, width = 310, height = 240, data type = uint8, stride = 320, skip = 0
          chanel_v: offset = 384000, width = 310, height = 240, data type = uint8, stride = 320, skip = 0

                  &lt;--------------------------- y-stride ---------------------------&gt;
                  &lt;----------------------- y-width -----------------------&gt;
          [index] 012345                                                  619      639
                  ||||||--------------------------------------------------|--------|
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                        Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                        Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                        Y%%%%%%%%%
          [data]  ......
                  &lt;-------- u-stride ----------&gt;
                  &lt;----- u-width -----&gt;
          [index] 307200              307509   307519
                  |-------------------|--------|
          [data]  UUUUUUUUUU...       U%%%%%%%%%
          [data]  UUUUUUUUUU...       U%%%%%%%%%
          [data]  UUUUUUUUUU...       U%%%%%%%%%
          [data]  ......
                  &lt;-------- v-stride ----------&gt;
                  &lt;- --- v-width -----&gt;
          [index] 384000              384309   384319
                  |-------------------|--------|
          [data]  VVVVVVVVVV...       V%%%%%%%%%
          [data]  VVVVVVVVVV...       V%%%%%%%%%
          [data]  VVVVVVVVVV...       V%%%%%%%%%
          [data]  ......
        </pre>
        <pre class='example highlight'>
          Example3: YUV420SP_NV12 image, width = 620, height = 480, stride = 640

          chanel_y: offset = 0, width = 620, height = 480, stride = 640, skip = 0
          chanel_u: offset = 307200, width = 310, height = 240, data type = uint8, stride = 640, skip = 1
          chanel_v: offset = 307201, width = 310, height = 240, data type = uint8, stride = 640, skip = 1

                  &lt;--------------------------- y-stride --------------------------&gt;
                  &lt;----------------------- y-width ----------------------&gt;
          [index] 012345                                                 619      639
                  ||||||-------------------------------------------------|--------|
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                       Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                       Y%%%%%%%%%
          [data]  YYYYYYYYYYYYYYYYYYYYYYYYYYYYY...                       Y%%%%%%%%%
          [data]  ......
                  &lt;--------------------- u-stride / v-stride --------------------&gt;
                  &lt;------------------ u-width + v-width -----------------&gt;
          [index] 307200(u-offset)                                       307819  307839
                  |------------------------------------------------------|-------|
          [index] |307201(v-offset)                                      |307820 |
                  ||-----------------------------------------------------||------|
          [data]  UVUVUVUVUVUVUVUVUVUVUVUVUVUVUV...                      UV%%%%%%%
          [data]  UVUVUVUVUVUVUVUVUVUVUVUVUVUVUV...                      UV%%%%%%%
          [data]  UVUVUVUVUVUVUVUVUVUVUVUVUVUVUV...                      UV%%%%%%%
                   ^            ^
                  u-skip        v-skip
        </pre>
        <pre class='example highlight'>
          Example4: DEPTH image, width = 640, height = 480, stride = 1280

          chanel_d: offset = 0, width = 640, height = 480, data type = uint16, stride = 1280, skip = 0

                  &lt;----------------------- d-stride ----------------------&gt;
                  &lt;----------------------- d-width -----------------------&gt;
          [index] 012345                                                  1280
                  ||||||--------------------------------------------------|
          [data]  DDDDDDDDDDDDDDDDDDDDDDDDDDDDD...                        D
          [data]  DDDDDDDDDDDDDDDDDDDDDDDDDDDDD...                        D
          [data]  DDDDDDDDDDDDDDDDDDDDDDDDDDDDD...                        D
          [data]  ......
        </pre>
        <section id='imageformatpixellayout-interface'>
          <h2>
            <code>ImageFormatPixelLayout</code> interface
          </h2>
          <dl class="idl" title=
          "[Exposed=(Window,Worker)] interface ChannelPixelLayout">
            <dt>
              readonly attribute unsigned long offset
            </dt>
            <dd>
              <p>
                The beginning position of this <a>channel</a>'s data (relative
                to the given <a>BufferSource</a> parameter of the mapDataInto()
                method.)
              </p>
            </dd>
            <dt>
              readonly attribute unsigned long width
            </dt>
            <dd>
              <p>
                The width of this channel. <a>Channel</a>s in an image format
                may have different width.
              </p>
            </dd>
            <dt>
              readonly attribute unsigned long height
            </dt>
            <dd>
              <p>
                The height of this channel. <a>Channel</a>s in an image format
                may have different height.
              </p>
            </dd>
            <dt>
              readonly attribute DataType dataType
            </dt>
            <dd>
              <p>
                The data type used to store one single <a>pixel value</a>.
              </p>
            </dd>
            <dt>
              readonly attribute unsigned long stride
            </dt>
            <dd>
              <p>
                The stride of this channel.
              </p>The stride is the number of bytes between the beging two
              consecutive raws in memory.
              <p>
                The total bytes of each raw plus the padding bytes of each raw.
              </p>
            </dd>
            <dt>
              readonly attribute unsigned long skip
            </dt>
            <dd>
              <p>
                This is used to describe how much bytes between two adjacent
                <a>pixel value</a>s in this channel.
              </p>
              <p>
                Possible values:
              </p>
              <ul>
                <li>zero: for <a>planar pixel layout</a>.
                </li>
                <li>a positive integer: for <a>interleaving pixel layout</a>.
                </li>
              </ul>
            </dd>
          </dl>
        </section>
        <section id='channelpixellayout-interface'>
          <h2>
            <code>ChannelPixelLayout</code> interface
          </h2>
          <dl class="idl" title=
          "[Exposed=(Window,Worker)] interface ImageFormatPixelLayout">
            <dt>
              readonly attribute sequence&lt;ChannelPixelLayout&gt; channels
            </dt>
            <dd>
              <p>
                Channel information of this image format. Each image format has
                at least one <a>channel</a>.
              </p>
            </dd>
          </dl>
        </section>
      </section>
      <section id='imagebitmap-interface-extensions'>
        <h2>
          <code>ImageBitmap</code> interface
        </h2>
        <dl class="idl" title=
        "[Exposed=(Window,Worker)] partial interface ImageBitmap">
          <dt>
            ImageFormat findOptimalFormat()
          </dt>
          <dd>
            <p>
              Find the best image format for receiving data.
            </p>
            <p>
              Return one of the <code>possibleFormats</code> or the empty
              string if no any format in the list is supported. If the
              <code>possibleFormats</code> is not given, then returns the most
              suitable image format for this ImageBitmap from all supported
              image formats.
            </p>
            <dl class="parameters">
              <dt>
                optional sequence&lt;ImageFormat&gt; possibleFormats
              </dt>
              <dd>
                A list of image formats that users can handler.
              </dd>
            </dl>
          </dd>
          <dt>
            long mappedDataLength()
          </dt>
          <dd>
            <p>
              Calculate the length of mapped data wile the image is represented
              in the given <code>format</code>.
            </p>
            <p>
              Throws if <code>format</code> is not supported.
            </p>
            <p>
              Return the length (in bytes) of image data that is represented in
              the given <code>format</code>.
            </p>
            <dl class="parameters">
              <dt>
                ImageFormat format
              </dt>
              <dd>
                The format that users want.
              </dd>
            </dl>
          </dd>
          <dt>
            Promise&lt;ImageFormatPixelLayout&gt; mapDataInto()
          </dt>
          <dd>
            <p>
              Makes a copy of the underlying image data in the given format
              <code>format</code> into the given <code>buffer</code> at offset
              <code>offset</code>, filling at most <code>length</code> bytes
              and returns an <a>ImageFormatPixelLayout</a> object which
              describes the pixel layout.
            </p>
            <p>
              Throws if <code>format</code> is not supported.
            </p>
            <p>
              Each time this method is invoked returns a new
              <a>ImageFormatPixelLayout</a> object.
            </p>
            <p>
              Return an <a>ImageFormatPixelLayout</a> object which describes
              the pixel layout.
            </p>
            <dl class="parameters">
              <dt>
                ImageFormat format
              </dt>
              <dd>
                The format that users want.
              </dd>
              <dt>
                BufferSource buffer
              </dt>
              <dd>
                A container for receiving the mapped image data.
              </dd>
              <dt>
                long offset
              </dt>
              <dd>
                The beginning position of the <code>buffer</code> to place the
                mapped data.
              </dd>
              <dt>
                long length
              </dt>
              <dd>
                The length of space in the <code>buffer</code> that could be
                filled.
              </dd>
            </dl>
          </dd>
        </dl>
      </section>
      <section id='imagebitmapfactories-interface-extensions'>
        <h2>
          <code>ImageBitmapFactories</code> interface
        </h2>
        <dl class="idl" title=
        "[NoInterfaceObject, Exposed=(Window,Worker)] partial interface ImageBitmapFactories">
        <dt>
            Promise&lt;ImageBitmap&gt; createImageBitmap()
          </dt>
          <dd>
            <p>
              Create an <code>ImageBitmap</code> from a
              <code>BufferSource</code> containg raw image data.
            </p>
            <dl class="parameters">
              <dt>
                BufferSource buffer
              </dt>
              <dd>
                A container of the raw image data.
              </dd>
              <dt>
                long offset
              </dt>
              <dd>
                The beginning position of the <code>buffer</code> where the raw
                image data is placed.
              </dd>
              <dt>
                long length
              </dt>
              <dd>
                The length of spaces in the <code>buffer</code> that the raw
                image data is palced.
              </dd>
              <dt>
                ImageFormat format
              </dt>
              <dd>
                The format of the raw image data.
              </dd>
              <dt>
                ImageFormatPixelLayout layout
              </dt>
              <dd>
                The pixel layout of the raw image data, which describes how the
                data is arranged in the given <code>buffer</code> as the given
                <code>format</code>.
              </dd>
            </dl>
          </dd>
          <dt>
            Promise&lt;ImageBitmap&gt; createImageBitmap()
          </dt>
          <dd>
            <p>
              Create an <code>ImageBitmap</code> from a
              <code>BufferSource</code> containg raw image data with a given
              cropping area.
            </p>
            <dl class="parameters">
              <dt>
                BufferSource buffer
              </dt>
              <dd>
                A container of the raw image data.
              </dd>
              <dt>
                long offset
              </dt>
              <dd>
                The beginning position of the <code>buffer</code> where the raw
                image data is placed.
              </dd>
              <dt>
                long length
              </dt>
              <dd>
                The length of spaces in the <code>buffer</code> that the raw
                image data is palced.
              </dd>
              <dt>
                ImageFormat format
              </dt>
              <dd>
                The format of the raw image data.
              </dd>
              <dt>
                ImageFormatPixelLayout layout
              </dt>
              <dd>
                The pixel layout of the raw image data, which describes how the
                data is arranged in the given <code>buffer</code> as the given
                <code>format</code>.
              </dd>
              <dt>
                long sx
              </dt>
              <dd>
                The x-coordinate of the starting point of the cropping
                rectangle.
              </dd>
              <dt>
                long sy
              </dt>
              <dd>
                The y-coordinate of the starting point of the cropping
                rectangle.
              </dd>
              <dt>
                long sw
              </dt>
              <dd>
                The width of the cropping rectangle.
              </dd>
              <dt>
                long sh
              </dt>
              <dd>
                The height of the cropping rectangle.
              </dd>
            </dl>
          </dd>
        </dl>
        <p>
          When the <code>createImageBitmap()</code> is invocked, the User Agent
          MUST run the following steps:
        </p>
        <ol>
          <li>If either the <em>sw</em> or <em>sh</em> arguments are specified
          but zero, Return a promise rejected with an IndexSizeError exception
          and abort these steps.
          </li>
          <li>If the <em>buffer</em> has been <code><a href=
          "http://www.w3.org/TR/html51/infrastructure.html#concept-transferable-neutered">
            neutered</a></code>, Return a promise rejected with an
            InvalidStateError exception and abort these steps.
          </li>
          <li>Create a new <code>ImageBitmap</code> object.
          </li>
          <li>Let the <code>ImageBitmap</code> object's bitmap data be the
          image data given by the <code>BufferSource</code> object,
          <code><a href=
          "http://www.w3.org/TR/html51/webappapis.html#cropped-to-the-source-rectangle">
            cropped to the source rectangle</a></code>.
          </li>
          <li>Return a new <code>Promise</code>, but continue running these
          steps <code><a href=
          "http://www.w3.org/TR/html51/infrastructure.html#in-parallel">in
          parallel</a></code>.
          </li>
          <li>
            <code><a href=
            "http://www.w3.org/TR/html51/infrastructure.html#concept-resolver-fulfill">
            Resolve</a></code> the <code>Promise</code> with the new
            <code>ImageBitmap</code> object as the value.
          </li>
        </ol>
      </section>
    </section>
    <section>
      <h2>
        Examples
      </h2>
      <h3>
        WorkerMonitor example:
      </h3>
      <p>
        This example demonstrates how to hook a <code>Worker</code> with a
        <code>MediaStreamTrack</code> and also shows how to use
        <code>postMessage</code> to communicate between main thread and worker
        thread. In this example, the role of control_worker.js is doing flow
        control. This script decide whether drop frame or not. If the system is
        not busy, the script will dispatch current frame to process_worker.js
        or it just drop the frame.
      </p>
      <h4>
        Main file javascript
      </h4>
      <pre class='example highlight'>
        playButton.onclick = function() {
          navigator.getUserMedia({video: true, audio: false} , onSuccess, onFail);
        }
        function onSuccess(stream) {
            localMediaStream = stream;
            initControlWorker();
            if (showWebCamVideo) {
                video.src = window.URL.createObjectURL(localMediaStream);
                video.play();
            }
        }
        function initControlWorker() {
          controlWorker = new Worker("control_worker.js");
          if (!!controlWorker) {
            var tracks = localMediaStream.getTracks();
            controlWorker.postMessage({"type":"set_trackid", "id":tracks[0].id});
            controlWorker.postMessage({"type":"init_process_worker"});
            tracks[0].addVideoMonitor(controlWorker);
          }
          controlWorker.onmessage = function(event) {
            if(event.data.type == "display_arraybuffer") {
              if (!isCanvasInitialized) {
                canvasResult.width = event.data.bitmap.width;
                canvasResult.height = event.data.bitmap.height;
                isCanvasInitialized = true;
              }
              // show result
              ctxResult.drawImage(event.data.bitmap, 0, 0);
            }
          };
        }
      </pre>
      <h4>
        control_worker.js
      </h4>
      <pre class='example highlight'>
        var trackid;
        var worker;
        var isProcessing = false;
        var processWorker;
        onmessage = function(event) {
          if (event.data.type == "init_process_worker") {
            processWorker = new Worker("process_worker.js");

            processWorker.onmessage = function(event) {
              if (event.data.type == "display_arraybuffer") {
                // send back to the main thread
                postMessage({"type":event.data.type,
                             "bitmap":event.data.bitmap});
                isProcessing = false;
              }
            };
          } else if (event.data.type == "set_trackid") {
            trackid = event.data.id;
          }
        }

        var frameNum = 0;
        var dropNum = 0;

        function showDropRate() {
          console.log("Drop rate = " + 100 * dropNum / frameNum + "%");
        }

        onvideoprocess = function(event) {
          if (frameNum % 100 == 1) {
            showDropRate();
          }

          frameNum++;
          if (isProcessing) {
            // drop this frame
            console.log("drop frame[" + frameNum + "]");
            dropNum++;
            return;
          } else {
            // console.log("process frame[" + frameNum + "]")
            isProcessing = true;
            // send to the process worker
            processWorker.postMessage({"type":"convert_color",
                                       "bitmap":event.inputImageBitmap});
          }
        };
      </pre>
      <h4>
        process_worker.js
      </h4>
      <pre class='example highlight'>
        function processOneFrame_RGBA(bitmap) {
          // Do what you want to do!
          ...
        }
        onmessage = function(event) {
          // do the invernt effect
          processOneFrame_RGBA(event.data.bitmap);

          // send back to the control worker
          postMessage({"type":"display_arraybuffer",
                       "bitmap":event.data.bitmap});
        };
      </pre>
      <h3>
        WorkerMonitor example in multiple workers:
      </h3>
      <p>
        This example is extended from previous "WorkerMonitor example". The
        main difference is this example create a WorkerPool object in
        control_worker.js. The script will reuse the workers in the pool. This
        mechanism full utilized the power of multi-core machine to reduce the
        frame drop rate.
      </p>
      <h4>
        Main file javascript
      </h4>
      <pre class='example highlight'>
        playButton.onclick = function() {
            navigator.getUserMedia({video: true, audio: false} , onSuccess, onFail);
        }
        stopButton.onclick = function() {
            isContinuous = false;
            video.pause();
            localMediaStream.stop();
        }
        function onSuccess(stream) {
            localMediaStream = stream;
            initControlWorker();
            if (showWebCamVideo) {
                video.src = window.URL.createObjectURL(localMediaStream);
                video.play();
            }
        }
        function onFail(e) {
            console.log('Cannot access WebCAM!', e);
        }
        function initControlWorker() {
          controlWorker = new Worker("control_worker.js");
          if (!!controlWorker) {
            var tracks = localMediaStream.getTracks();
            controlWorker.postMessage({"type":"set_trackid", "id":tracks[0].id});
            controlWorker.postMessage({"type":"init_process_worker"});
            tracks[0].addVideoMonitor(controlWorker);
          }
          controlWorker.onmessage = function(event) {
            if (event.data.taskType == "DisplayTask") {
              if (!isCanvasInitialized) {
                canvasResult.width = event.data.bitmap.width;
                canvasResult.height = event.data.bitmap.height;
                isCanvasInitialized = true;
              }
              ctxResult.drawImage(event.data.bitmap, 0, 0);
            }
          };
        }
      </pre>
      <h4>
        control_worker.js
      </h4>
      <pre class='example highlight'>
        /*
         * WorkerPool
         */
        function WorkerPool() {
          this.avaialableQueue = [];
          this.workingQueue = [];
        }

        WorkerPool.prototype.Init = function(script, numOfWorkers) {
          for (var i = 0; i &lt; numOfWorkers; ++i) {
            this.avaialableQueue.push(new ProcessWorker(i, script, this));
          }
        }

        WorkerPool.prototype.Process = function(frameNum, bitmap) {
          var task = new ProcessTask(frameNum, bitmap);
          return this.Dispatch(task);
        }

        WorkerPool.prototype.Dispatch = function(task) {
          if (task.taskType == ProcessTask.TASK_TYPE) {
            if (this.avaialableQueue.length &gt; 0) {
              var processWorker = this.avaialableQueue.shift();
              processWorker.worker.postMessage(task);
              this.workingQueue.push(processWorker);
              return true;
            }
            else {
              console.log("No availabe ProcessWorker now, drop this frame[" + task.frameNum + "]......");
              return false;
            }
          } else {
            console.log("Cannot handle " + task.taskType);
            return false;
          }
        }

        WorkerPool.prototype.Display = function() {
          while(this.workingQueue.length &gt; 0) {
            if (!!(this.workingQueue[0].displayTask)) {
              var processWorker = this.workingQueue.shift();
              postMessage(processWorker.displayTask);
              processWorker.displayTask = null;
              this.avaialableQueue.push(processWorker);
            } else {
              break;
            }
          }
        }

        WorkerPool.prototype.FindProcessWorker = function(worker) {
          for (var i = 0; i &lt; this.workingQueue.length; ++i) {
            if (this.workingQueue[i].worker == worker) {
              return this.workingQueue[i];
            }
          }
        }

        /*
         * ProcessWorker
         */
        function ProcessWorker(id, script, pool) {
          this.workerID = id;
          this.workerPool = pool
          this.displayTask = null;
          this.worker = new Worker(script);
          this.worker.wraper = this;
          this.worker.onmessage = function(event) {
            var processWorker = workerPool.FindProcessWorker(this);
            processWorker.displayTask = new DisplayTask(event.data.frameNum, event.data.bitmap);
            workerPool.Display();
            // workerPool.PrintInfo("in worker.onmessage()");
          }
        }

        /*
         * Task, ProcessTask, DisplayTask
         */
        function Task(type, frameNum) {
          this.taskType = type;
          this.frameNum = frameNum;
        }

        function ProcessTask(frameNum, bitmap) {
          Task.call(this, ProcessTask.TASK_TYPE, frameNum);
          this.bitmap = bitmap;
        }
        ProcessTask.prototype = Object.create(Task.prototype);
        ProcessTask.prototype.constructor = ProcessTask;
        ProcessTask.TASK_TYPE = "ProcessTask";

        function DisplayTask(frameNum, bitmap) {
          Task.call(this, DisplayTask.TASK_TYPE, frameNum);
          this.bitmap = bitmap;
        }
        DisplayTask.prototype = Object.create(Task.prototype);
        DisplayTask.prototype.constructor = DisplayTask;
        DisplayTask.TASK_TYPE = "DisplayTask";
        var numOfWorkers = 2; // initialize the WorkerPool whit this number of workers.
        var workerPool;
        var trackid;
        var frameNum = 0;
        var dropNum = 0;

        function showDropRate() {
          console.log("Drop rate = " + 100 * dropNum / frameNum + "%");
        }


        onmessage = function(event) {
          if (event.data.type == "init_process_worker") {
            workerPool = new WorkerPool();
            workerPool.Init("process_worker.js", numOfWorkers);
          } else if (event.data.type == "set_trackid") {
            trackid = event.data.id;
          }
        }

        onvideoprocess = function(event) {
          if (frameNum % 100 == 1) {
            showDropRate();
          }

          if (!workerPool.Process(frameNum++, event.inputImageBitmap)) {
            dropNum++;
          }
        };
      </pre>
      <h4>
        process_worker.js
      </h4>
      <pre class='example highlight'>
        function processOneFrame_RGBA(bitmap) {
          // Do what you want to do!
          ...
        }
        onmessage = function(event) {
          // do the invernt effect
          processOneFrame_RGBA(event.data.bitmap);

          // send back to the control worker
          postMessage({"type":"display_arraybuffer",
               "frameNum":event.data.frameNum,
               "bitmap":event.data.bitmap});
        };
      </pre>
      <h3>
        WorkerProcessor example:
      </h3>
      <p>
        This example shows how to process and display the processed
        <a>MediaStreamTrack</a>. Be careful, the <code>Worker</code> is
        appended into the new <a>MediaStreamTrack</a>, not original one.
      </p>
      <h4>
        Main file javascript
      </h4>
      <pre class='example highlight'>
        playButton.onclick = function() {
            navigator.getUserMedia({video: true, audio: false} , onSuccess, onFail);
        }
        stopButton.onclick = function() {
            isContinuous = false;
            video.pause();
            localMediaStream.stop();
        }
        function onSuccess(stream) {
            localMediaStream = stream;
            initControlWorker();
            if (showWebCamVideo) {
                video.src = window.URL.createObjectURL(localMediaStream);
                video.play();
            }
        }
        function onFail(e) {
            console.log('Cannot access WebCAM!', e);
        }
        function initControlWorker() {
          controlWorker = new Worker("processor.js");
          if (!!controlWorker) {
            var tracks = localMediaStream.getTracks();
            controlWorker.postMessage({"type":"set_trackid", "id":tracks[0].id});
            var newtrack = tracks[0].addVideoProcessor(controlWorker);
            // link to the result video
            resultVideo.mozSrcObject = new MediaStream([newtrack]);
            resultVideo.play();
          }
          controlWorker.onmessage = function(event) {
            if(event.data.type == "display_arraybuffer") {
              if (!isCanvasInitialized) {
                resultVideo.width = event.data.imageWidth;
                resultVideo.height = event.data.imageHeight;
                isCanvasInitialized = true;
              }
            }
          };
        }
      </pre>
      <h4>
        processor.js
      </h4>
      <pre class='example highlight'>
        function processOneFrame_Invert(inputBitmap, outputBitmap) {
          var bitmap = inputBitmap;
          var format = bitmap.findOptimalFormat();
          format = "RGBA32"; // force it to be RGBA32, do conversion in Gecko
          var length = bitmap.mappedDataLength(format);

          if (format != bitmapFormat || length != bitmapBufferLength) {
            bitmapFormat = format;
            bitmapBufferLength = length;
            bitmapBuffer = new ArrayBuffer(bitmapBufferLength);
            bitmapBufferView = new Uint8ClampedArray(bitmapBuffer, 0, bitmapBufferLength);
          }
          var bitmapPixelLayout = bitmap.mapDataInto(bitmapFormat, bitmapBuffer, 0, bitmapBufferLength);

          if (!isInitialized) {
            rgbaBufferLength = bitmapBufferLength;
            rgbaBuffer = new ArrayBuffer(rgbaBufferLength);
            rgbaBufferView = new Uint8ClampedArray(rgbaBuffer, 0, rgbaBufferLength);

            isInitialized = true;
          }

          // convert YUV to Gray or RGBA
          for (var i = 0; i &lt; bitmap.height; ++i) {
            for (var j = 0; j &lt; bitmap.width; ++j) {

              /*
               *  do invert effect
               */
              var index = bitmap.width * i + j;
              rgbaBufferView[index * 4 + 0] = 255 - bitmapBufferView[index * 4 + 0];
              rgbaBufferView[index * 4 + 1] = 255 - bitmapBufferView[index * 4 + 1];
              rgbaBufferView[index * 4 + 2] = 255 - bitmapBufferView[index * 4 + 2];
              rgbaBufferView[index * 4 + 3] = bitmapBufferView[index * 4 + 3]; // no change in the appha channel
            }
          }

          // write back to event outputImageBitmap
          outputBitmap.setDataFrom("RGBA32", rgbaBuffer, 0, rgbaBufferLength,
                                   bitmap.width, bitmap.height, bitmapPixelLayout.channels[0].stride);
        }
        onvideoprocess = function(event) {
          processOneFrame_Invert(event.inputImageBitmap, event.outputImageBitmap);
        }
      </pre>
    </section>
    <section class='appendix'>
      <h2>
        Acknowledgements
      </h2>
      <p>
        Thanks to Robert O'Callahan for his idea of this design.
      </p>
    </section>
  </body>
</html>
